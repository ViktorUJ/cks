# Solution
````
kubectl config use-context cluster1-admin@cluster1
````
login to control plane
```
# k  get no
NAME             STATUS   ROLES           AGE   VERSION
ip-10-10-1-131   Ready    control-plane   37m   v1.34.1

```
```
ssh ip-10-10-1-131
```

disable `anonymous-auth`  and `authorization-mode=AlwaysAllow`
```
# sudo -i 
# vim /etc/kubernetes/manifests/kube-apiserver.yaml
........
spec:
  containers:
  - command:
    - kube-apiserver
    ...
    - --anonymous-auth=false                       # update it
    - --authorization-mode=Node,RBAC               # update it
......    
```
restart kube-apiserver
```
systemctl restart kubelet
crictl ps | grep kube-apiserver

```
verify the changes
```
# curl -k https://127.0.0.1:6443/api/v1/namespaces

{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "Unauthorized",
  "reason": "Unauthorized",
  "code": 401
}
```
find clusterrolebinding  with `system:anonymous` 
``` 
# k get clusterrolebinding  -o wide | grep anonymous

anonymous-binding     ClusterRole/anonymous      61m   system:anonymous                                                           

```
delete the clusterrolebinding and clusterrole
``` 
# k delete clusterrolebinding anonymous-binding
# k delete clusterrole anonymous
```
find rolebinding  with `system:anonymous`
```` 
# k get rolebinding -A -o wide | grep ano
kube-public   kubeadm:bootstrap-signer-clusterinfo                Role/kubeadm:bootstrap-signer-clusterinfo             81m   system:anonymous                                                                                      

````

It is ok .  no need to delete it.



