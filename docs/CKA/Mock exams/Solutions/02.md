# 02

Solutions for CKA Mock exam #02

[Video Solution](https://youtu.be/ia6Vw_BR-L0?feature=shared)

## 01

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k get po  -n dev-1  --show-labels

k get po  -n dev-1  -l team=finance

k top po -n dev-1 -l team=finance --sort-by memory

k label pod {pod_name with max memory usage} -n dev-1 usage=max
```

## 02

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k run util --image busybox:1.36 -n dev --command sleep 3600
k get po util -n dev
```

## 03

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k create ns team-elephant
k get ns  team-elephant
```

## 04

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k get no -l disk=ssd
k run alpine --image alpine:3.15 -o yaml --dry-run=client --command sleep 6000  >4.yaml
```

```yaml
# vim 4.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: alpine
  name: alpine
spec:
  nodeSelector:      # add
     disk: ssd       # add
  containers:
  - command:
    - sleep
    - "6000"
    image: alpine:3.15
    name: alpine
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

```

```sh
k apply -f 4.yaml
```

## 05

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k create deployment web-app --image viktoruj/ping_pong:latest --replicas 2 --port 8080 -o yaml  --dry-run=client  >5.yaml
```

```sh
# vim 5.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: web-app
  name: web-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web-app
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: web-app
    spec:
      containers:
      - image: viktoruj/ping_pong:latest
        name: ping-pong-2cwhf
        ports:
        - containerPort: 8080
          name: http-web      # add it
        resources: {}
status: {}
```

```sh
k apply -f 5.yaml
```

## 06

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k expose  deployment web-app -n dev-2 --port 8080 --type NodePort --name web-app-svc
k get svc -n dev-2
```

## 07

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k run web-srv --image viktoruj/ping_pong:latest --dry-run=client -o yaml  > 7.yaml
```

```yaml
# vim 7.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: web-srv
  name: web-srv
spec:
  containers:
  - image: viktoruj/ping_pong:latest
    name: app1    # change from web-srv  to app1
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

```yaml
k apply -f 7.yaml

k get po web-srv
```

## 08

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k get deployment redis-node  -n db-redis

k scale deployment redis-node  -n db-redis  --replicas 1

k get deployment redis-node  -n db-redis
```

## 09

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
echo 'kubectl get po -n dev-2 -o json --context cluster1-admin@cluster1' >/var/work/artifact/9.sh
bash /var/work/artifact/9.sh
```

## 10

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k get no -l node_name=node_2
# ssh to worker node
sudo mkdir /pv/analytics -p
sudo chmod  777 -R /pv/analytics
exit
```

```yaml
# vim 10.yaml
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-analytics
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/pv/analytics"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-analytics
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi
---
apiVersion: v1
kind: Pod
metadata:
  name: analytics
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: pvc-analytics
  nodeSelector:
    node_name: node_2
  containers:
    - name: task-pv-container
      image: busybox
      command: ["sleep","60000"]
      volumeMounts:
        - mountPath: "/pv/analytics"
          name: task-pv-storage
```

```sh
k apply -f 10.yaml
```

## 11

Use correct context

```sh
kubectl config use-context cluster2-admin@cluster2
```

Drain master node

```sh
kubectl  drain {master node name}  --ignore-daemonsets
```

Check kubeadm version

```sh
ssh  {master node name}

sudo su

kubeadm version
```

Check kubelet version

```sh
kubelet --version
```

Install and update packages

```sh
apt update

apt-cache madison kubeadm

apt-mark unhold kubeadm

apt install kubeadm=1.28.4-1.1  -y
apt-mark hold kubeadm
```

Update control plane

```sh
kubeadm upgrade plan

kubeadm upgrade apply v1.28.4
```

Update kubelet and kubectl

```sh
apt-mark unhold kubelet kubectl
apt install kubelet=1.28.4-1.1   kubectl=1.28.4-1.1 -y
apt-mark hold kubelet kubectl

service kubelet restart
service kubelet status

exit
exit
```

Uncordon master node

```sh
kubectl  uncordon {master node name}
```

Drain node

```sh
k drain {worker node } --ignore-daemonsets
```

Ssh to worker node
Update kubeadm

```sh
ssh {worker node}
sudo su

apt update
apt-mark unhold kubeadm
apt install kubeadm=1.28.4-1.1 -y
apt-mark hold kubeadm
kubeadm upgrade node
```

Update kubelet and kubectl

```sh
apt-mark unhold kubectl kubelet
apt install kubelet=1.28.4-1.1  kubectl=1.28.4-1.1 -y
apt-mark hold kubectl kubelet
service kubelet restart
service kubelet status
```

Uncordon worker node

```sh
kubectl  uncordon {worker  node name}
```

Check nodes

```sh
kubectl get no
````

## 12

```sh
kubectl config use-context cluster1-admin@cluster1
```

```yaml
#vim 12.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: cat
  namespace: cat
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /cat
        pathType: Prefix
        backend:
          service:
            name: cat
            port:
              number: 80

```

```sh
k apply -f 12.yaml

curl cka.local:30102/cat
```

## 13

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k get ns team-elephant

k create ns team-elephant

k create serviceaccount pod-sa --namespace team-elephant

k create role pod-sa-role -n team-elephant --resource pods --verb list,get

k create rolebinding  pod-sa-roleBinding -n team-elephant --role pod-sa-role --serviceaccount team-elephant:pod-sa

k run pod-sa --image viktoruj/cks-lab -n team-elephant  -o yaml --dry-run=client  --command sleep 60000  >13.yaml
```

```yaml
# vim 13.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod-sa
  name: pod-sa
  namespace: team-elephant
spec:
  serviceAccountName: pod-sa   # <--- Add ServiceAccountName here
  containers:
  - command:
    - sleep
    - "60000"
    image: viktoruj/cks-lab
    name: pod-sa
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

```sh
k apply -f 13.yaml

k get po -n team-elephant
```

```sh
k  auth can-i list  pods --as=system:serviceaccount:team-elephant:pod-sa --namespace=team-elephant

yes

k  auth can-i delete  pods --as=system:serviceaccount:team-elephant:pod-sa --namespace=team-elephant

no
```

(Optional) Check permissions from pod (not nesesary )

```sh

kubectl exec pod-sa  -n team-elephant  --context cluster1-admin@cluster1  -- sh -c 'curl  GET https://kubernetes.default/api/v1/namespaces/team-elephant/pods/  -s -H "Authorization: Bearer $(cat /run/secrets/kubernetes.io/serviceaccount/token)" -k'
```

## 14

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k get ns team-elephant

k create deployment team-elephant-ds --image viktoruj/ping_pong -o yaml --dry-run=client -n team-elephant > 14.yaml
```

```yaml
# vim 14.yaml
apiVersion: apps/v1
kind: DaemonSet  # update to DaemonSet
metadata:
  creationTimestamp: null
  labels:
    app: team-elephant-ds
    team: team-elephant                           # add it
    env: dev                                      # add it
  name: team-elephant-ds
  namespace: team-elephant
spec:
#  replicas: 1                                    # comment or delete it
  selector:
    matchLabels:
      app: team-elephant-ds
#   strategy: {}                                   # comment or delete it
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: team-elephant-ds
        team: team-elephant # add it
        env: dev                                     # add it
    spec:
      tolerations:                                   # add it
      - key: node-role.kubernetes.io/control-plane   # add it
        effect: "NoSchedule"                         # add it
      containers:
      - image: viktoruj/ping_pong
        name: ping-pong-q5cxp
        resources:
          requests:                                  # add it
            cpu: 50m                                 # add it
            memory: 50Mi                             # add it
status: {}
```

```sh
k apply -f 14.yaml
k get po -n team-elephant -o wide
```

## 15

```sh
kubectl config use-context cluster1-admin@cluster1
```

```yaml
#k edit deployment legacy-app -n legacy
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: legacy-app
  name: legacy-app
  namespace: legacy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: legacy-app
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: legacy-app
    spec:
      volumes:                                                                  # add it
        - emptyDir:                                                             # add it
            sizeLimit: 500Mi                                                    # add it
          name: logs                                                            # add it
      containers:
      - image: viktoruj/ping_pong
        name: app1
        volumeMounts:
        - mountPath: /log
          name: logs
        env:
        - name: SERVER_NAME
          value: "app1"
        - name: SRV_PORT
          value: "8081"
        - name: METRIC_PORT
          value: "9092"
        - name: LOG_PATH
          value: /log/logs1.txt
        - name: ENABLE_OUTPUT
          value: "false"
      - image: viktoruj/ping_pong
        name: app2
        volumeMounts:                                                             # add it
        - mountPath: /log                                                         # add it
          name: logs                                                              # add it
        env:
        - name: SERVER_NAME
          value: "app2"
        - name: SRV_PORT
          value: "8082"
        - name: METRIC_PORT
          value: "9092"
        - name: LOG_PATH
          value: /log/logs2.txt
        - name: ENABLE_OUTPUT
          value: "false"
      - image: viktoruj/cks-lab                                                    # add it
        name: log                                                                  # add it
        command: ["tail","-f","-n","100", "/log/logs1.txt","-f","/log/logs2.txt"]  # add it
        volumeMounts:                                                              # add it
        - mountPath: /log                                                          # add it
          name: logs                                                               # add it
```

```sh
# check logs

k exec  checker -n legacy -- sh -c 'curl legacy-app:8081/test_app1'
k exec  checker -n legacy -- sh -c 'curl legacy-app:8082/test_app2'

k logs  -l app=legacy-app  -n legacy  -c log
```

## 16

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
echo 'kubectl get events --sort-by=".metadata.creationTimestamp" -A --context cluster1-admin@cluster1' >/var/work/artifact/16.sh
bash /var/work/artifact/16.sh

```

## 17

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
echo 'kubectl api-resources --namespaced=true --context cluster1-admin@cluster1 ' > /var/work/artifact/17.sh
bash /var/work/artifact/17.sh
```

## 18

```sh
kubectl config use-context cluster3-admin@cluster3
```

```sh
k get no

NAME             STATUS     ROLES           AGE     VERSION
ip-10-2-27-136   NotReady   <none>          9m15s   v1.29.0
ip-10-2-31-152   Ready      control-plane   9m37s   v1.29.0
```

```sh
ssh ip-10-2-27-136
```

```sh
sudo su
```

```sh
$ kubelet --version
Kubernetes v1.29.0

$ service kubelet status

● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; disabled; vendor preset: enabled)
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf, 20-labels-taints.conf
     Active: inactive (dead)
       Docs: https://kubernetes.io/docs/

Jan 12 08:33:05 ip-10-2-27-136 kubelet[5252]: I0112 08:33:05.996524    5252 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started f>
Jan 12 08:33:05 ip-10-2-27-136 kubelet[5252]: I0112 08:33:05.996547    5252 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started f>
Jan 12 08:33:05 ip-10-2-27-136 kubelet[5252]: I0112 08:33:05.996570    5252 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started f>
Jan 12 08:33:05 ip-10-2-27-136 kubelet[5252]: I0112 08:33:05.996592    5252 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started f>
Jan 12 08:33:05 ip-10-2-27-136 kubelet[5252]: I0112 08:33:05.996619    5252 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started f>
Jan 12 08:33:05 ip-10-2-27-136 kubelet[5252]: I0112 08:33:05.996641    5252 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started f>
Jan 12 08:33:06 ip-10-2-27-136 systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
Jan 12 08:33:06 ip-10-2-27-136 kubelet[5252]: I0112 08:33:06.681646    5252 dynamic_cafile_content.go:171] "Shutting down controller" name="client-ca-bundle::/et>
Jan 12 08:33:06 ip-10-2-27-136 systemd[1]: kubelet.service: Succeeded.
Jan 12 08:33:06 ip-10-2-27-136 systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
```

```sh
systemctl enable kubelet

Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /lib/systemd/system/kubelet.service.
```

```sh
systemctl start  kubelet
systemctl status kubelet

exit
exit
```

```sh
ubuntu@worker:~> k get no
NAME             STATUS   ROLES           AGE    VERSION
ip-10-2-27-136   Ready    <none>          101m   v1.29.0
ip-10-2-31-152   Ready    control-plane   102m   v1.29.0
```

## 19

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k run stat-podv --image viktoruj/ping_pong:latest -o yaml --dry-run=client > 19.yaml
```

```yaml
# vim 19.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: stat-podv
  name: stat-podv
spec:
  containers:
  - image: viktoruj/ping_pong:latest
    name: stat-podv
    resources:
      requests:                                  # add it
        cpu: 100m                                # add it
        memory: 128Mi                            # add it
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

```sh
k get no
scp 19.yaml {controlPlane}:/tmp/19.yaml

ssh {controlPlane}
```

```sh
sudo su

mv /tmp/19.yaml /etc/kubernetes/manifests/
```

```sh
$ k get po

NAME                      READY   STATUS    RESTARTS   AGE
stat-podv-ip-10-2-11-20   1/1     Running   0          5s
```

```sh
# exit to worker node
exit
exit
```

```sh
k expose pod stat-podv-{controlPlane node name} --port 8080 --type NodePort --name stat-pod-svc
```

```sh
k edit svc stat-pod-svc
```

```yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2024-01-11T19:06:33Z"
  labels:
    run: stat-podv
  name: stat-pod-svc
  namespace: default
  resourceVersion: "2638"
  uid: 951e70b8-5238-4aa2-98d9-de242718db71
spec:
  clusterIP: 10.96.17.72
  clusterIPs:
  - 10.96.17.72
  externalTrafficPolicy: Cluster
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - nodePort: 30084                  # update it to 30084
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    run: stat-podv
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}

```

```sh
$ k get svc stat-pod-svc

NAME           TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
stat-pod-svc   NodePort   10.96.17.72   <none>        8080:30084/TCP   2m16s
```

```sh
$ curl {controlPlane ip}:30084

Server Name: ping_pong_server
URL: http://ip-10-2-11-20:30084/
Client IP: 10.2.11.20
Method: GET
Protocol: HTTP/1.1
Headers:
User-Agent: curl/7.68.0
Accept: */*
```

## 20

```sh
kubectl config use-context cluster4-admin@cluster4
```

```sh
k get no

ssh {controlPlane}
```

```sh
sudo su

ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  snapshot save /var/work/tests/artifacts/20/etcd-backup.db

# stop api and etcd

mkdir /etc/kubernetes/tmp
mv /etc/kubernetes/manifests/* /etc/kubernetes/tmp/


# start etcd
mv /etc/kubernetes/tmp/etcd.yaml  /etc/kubernetes/manifests/

crictl ps | grep etcd

rm -rf /var/lib/etcd
ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --data-dir=/var/lib/etcd \
  snapshot restore  /var/work/tests/artifacts/20/etcd-backup_old.db
```

```sh
service kubelet restart

# start all static pods

mv /etc/kubernetes/tmp/* /etc/kubernetes/manifests/
```

```sh
# check  pod  kube-system
k get po -n kube-system


crictl ps

# delete old containers

crictl stop {old container id }

k get po -n kube-system
```

## 21

```sh
kubectl config use-context cluster5-admin@cluster5
```

```yaml
# vim 21_deny.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: prod-db

spec:
  podSelector: {}
  policyTypes:
  - Ingress
```

```sh
k apply -f 21_deny.yaml
```

```sh
k get ns --show-labels
````

```yaml
# vim 21_allow.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-policy
  namespace: prod-db
spec:
  podSelector:
    matchLabels: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: prod
        - namespaceSelector:
            matchLabels:
              name: stage
          podSelector:
            matchLabels:
              role: db-connect

        - podSelector:
            matchLabels:
              role: db-external-connect
          namespaceSelector: {}
```

```sh
k apply -f 21_allow.yaml
```
