# 01

Solutions for CKA Mock exam #01

[Video Solution](https://youtu.be/IZsqAPpbBxM?feature=shared)

## 01

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k run nginx-pod --image nginx:alpine
```

## 02

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
 k run messaging --image redis:alpine -l tier=msg
```

## 03

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k create ns apx-x9984574
```

## 04

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
mkdir /var/work/tests/artifacts/4/ -p
k get no -o json > /var/work/tests/artifacts/4/nodes.json
```

## 05

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k expose pod messaging --port 6379 --name messaging-service
```

## 06

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k create  deployment hr-web-app --image  nginx:alpine --replicas 2
```

## 07

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k get no
k run static-busybox --image busybox -o yaml --dry-run=client -l pod-type=static-pod --command sleep 60000 >7.yaml
scp 7.yaml {control_plane}:/tmp/
```

*ssh to control_plane node*

```sh
sudo cp /tmp/7.yaml /etc/kubernetes/manifests/
exit

k get po -l pod-type=static-pod
```

## 08

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k create ns finance
k run temp-bus -n finance --image redis:alpine
```

## 09

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
mkdir -p /var/work/tests/artifacts/9
k get no  -o jsonpath='{range .items[*]}{.status.nodeInfo.osImage}{"\n"}' >/var/work/tests/artifacts/9/os.json
```

## 10

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k run multi-pod --image nginx --env name=alpha -o yaml --dry-run=client > 10.yaml
```

```yaml
# vim 10.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: multi-pod
  name: multi-pod
spec:
  containers:
  - env:
    - name: name
      value: alpha
    image: nginx
    name: alpha
  - env:
    - name: name
      value: beta
    image: busybox
    name: beta
    command: ["sleep","4800"]

    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

```sh
k apply -f 10.yaml
```

## 11

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k expose deployment hr-web-app --port 80 --type NodePort  --name hr-web-app-service
k edit svc hr-web-app-service  # change NodePort number to 30082
```

## 12

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k get no -l node_name=node_2
# ssh to worker node
sudo mkdir /pv/analytics -p
sudo chmod  777 -R /pv/analytics
exit
```

```yaml
# vim 12.yaml

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-analytics
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/pv/analytics"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-analytics
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi

---

apiVersion: v1
kind: Pod
metadata:
  name: analytics
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: pvc-analytics
  nodeSelector:
    node_name: node_2
  containers:
    - name: task-pv-container
      image: busybox
      command: ["sleep","60000"]
      volumeMounts:
        - mountPath: "/pv/analytics"
          name: task-pv-storage
```

```sh
k apply -f 12.yaml
```

## 13

You can use this page as a reference: https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k get no
ssh {control-plane}
```

```sh
sudo ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  member list

sudo mkdir /var/work/tests/artifacts/13/ -p

sudo ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  snapshot save  /var/work/tests/artifacts/13/etcd-backup.db
```

## 14

```sh
kubectl config use-context cluster1-admin@cluster1
```

```yaml
# vim 14.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: redis-storage
  name: redis-storage
spec:
  containers:
  - image: redis:alpine
    name: redis-storage
    volumeMounts:
    - mountPath: /data/redis
      name: data
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  volumes:
  - name: data
    emptyDir:
      sizeLimit: 500Mi
status: {}
```

```sh
k apply -f 14.yaml
```

## 15

```sh
kubectl config use-context cluster1-admin@cluster1
```

```yaml
# vim 15.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: super-user-pod
  name: super-user-pod
spec:
  containers:
  - command:
    - sleep
    - "4800"
    image: busybox:1.28
    name: super-user-pod
    resources: {}
    securityContext:
      capabilities:
        add: ["SYS_TIME"]

  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

```sh
k apply -f 15.yaml
```

## 16

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k  create deployment nginx-deploy --image=nginx:1.16 --dry-run=client -o yaml > 16.yaml
k  apply -f 16.yaml --record
k  set image deployment/nginx-deploy nginx=nginx:1.17 --record
k  rollout history deployment nginx-deploy
```

## 17

Please check the following kubernetes docs page: https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
openssl genrsa -out myuser.key 2048
openssl req -new -key myuser.key -out myuser.csr
```

```sh
cat <<EOF > CSR.yaml
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: john-developer # add
spec:
  request: $(cat myuser.csr | base64 | tr -d "\n")
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth
  - digital signature
  - key encipherment
EOF
```

```sh
k create ns development
k apply -f  CSR.yaml
k get csr
k certificate approve john-developer
k create role developer --resource=pods --verb=create,list,get --namespace=development
k  create rolebinding developer-role-binding --role=developer --user=john --namespace=development
k  auth can-i update pods --as=john --namespace=development
```

## 18

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k create sa pvviewer
k create clusterrole pvviewer-role --verb list,get --resource PersistentVolumes
k create clusterrolebinding pvviewer-role-binding --clusterrole pvviewer-role --serviceaccount default:pvviewer
```

```sh
# vim 18.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pvviewer
  name: pvviewer
spec:
  containers:
  - image: viktoruj/cks-lab:latest
    name: pvviewer
    command: ["sleep","60000"]
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  serviceAccountName: pvviewer
status: {}
```

```sh
k apply -f  18.yaml
```

## 19

```sh
kubectl config use-context cluster1-admin@cluster1
```

```yaml
# vim 19.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: non-root-pod
  name: non-root-pod
spec:
  securityContext:
    runAsUser: 1000
    fsGroup: 2000
  containers:
  - image: redis:alpine
    name: non-root-pod
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

```sh
k apply -f 19.yaml
```

## 20

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k create ns prod-apps
k create secret generic  prod-secret -n prod-apps --from-literal var1=aaa --from-literal var2=bbb

echo "test config" > config.yaml
k create configmap prod-config -n prod-apps --from-file config.yaml

k run prod-app --image viktoruj/cks-lab:latest -o yaml --dry-run=client -n prod-apps --command sleep 60000 >20.yaml
```

```yaml
# vim 20.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: prod-app
  name: prod-app
  namespace: prod-apps
spec:
  containers:
  - command:
    - sleep
    - "60000"
    image: viktoruj/cks-lab:latest
    name: app1
    envFrom:
    - secretRef:
        name: prod-secret
    volumeMounts:
    - name: config
      mountPath: "/app/configs"
      readOnly: true
    resources: {}

  - command:
    - sleep
    - "60000"
    image: viktoruj/cks-lab:latest
    name: app2
    volumeMounts:
    - name: secret
      mountPath: "/app/secrets"
      readOnly: true
    resources: {}

  dnsPolicy: ClusterFirst
  restartPolicy: Always
  volumes:
  - name: config
    configMap:
      name: prod-config
  - name: secret
    secret:
     secretName: prod-secret
```

```sh
k apply -f  20.yaml
k get po -n prod-apps
```

References:
https://kubernetes.io/docs/concepts/configuration/configmap/
https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/

## 21

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
mkdir -p /var/work/tests/artifacts/21
k run nginx-resolver --image=nginx
k expose pod nginx-resolver --name=nginx-resolver-service --port=80 --target-port=80 --type=ClusterIP
```

```sh
# wait pod <nginx-resolver> - ready status
k run test-nslookup --image=busybox:1.28 --rm -it --restart=Never -- nslookup nginx-resolver-service > /var/work/tests/artifacts/21/nginx.svc

pod_ip=$( kubectl get po nginx-resolver -o jsonpath='{.status.podIP}' | sed 's/\./-/g' )
k run test-nslookup --image=busybox:1.28 --rm -it --restart=Never -- nslookup $pod_ip.default.pod > /var/work/tests/artifacts/21/nginx.pod
```

## 22

Use the correct context.

```sh
kubectl config use-context cluster2-admin@cluster2

k get no
```

Drain controlaplane(master) node.

```sh
kubectl  drain {master node name}  --ignore-daemonsets
```

```sh
ssh {master node name}
```

```sh
sudo su
```

Check kubeadm version.

```sh
kubeadm version
```

Check kubelet version.

```sh
kubelet --version
```

Update, install the packages.

```sh
apt update
apt-mark unhold kubeadm

apt install kubeadm=1.29.1-1.1  -y
apt-mark hold kubeadm
```

Update control plane.

```sh
kubeadm upgrade plan

kubeadm upgrade apply v1.29.1
```

Update kubelet and kubectl.

```sh
apt-mark unhold kubelet kubectl
apt install kubelet=1.29.1-1.1   kubectl=1.29.1-1.1
apt-mark hold kubelet kubectl

service kubelet restart
service kubelet status
```

```sh
# exit to worker PC
exit
exit
```

Uncordon master node

```sh
kubectl  uncordon {master node name}
```

Drain worker node

```sh
kubectl drain {worker node} --ignore-daemonsets
```

Ssh to worker node and update kubeadm

```sh
apt update
apt-mark unhold kubeadm
apt install kubeadm=1.29.1-1.1
apt-mark hold kubeadm
kubeadm upgrade node
```

Update kubelet and kubectl

```sh
apt-mark unhold kubectl kubelet
apt install kubelet=1.29.1-1.1  kubectl=1.29.1-1.1 -y
apt-mark hold kubectl kubelet
service kubelet restart
service kubelet status
```

Uncordon worker node

```sh
kubectl  uncordon {worker  node name}
```

Check nodes

```sh
kubectl get no
````

## 23

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
# vim 23_deny.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: prod-db

spec:
  podSelector: {}
  policyTypes:
  - Ingress
```

```sh
k apply -f 23_deny.yaml
```

```sh
k get ns --show-labels
```

```yaml
# vim 23_allow.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-policy
  namespace: prod-db
spec:
  podSelector:
    matchLabels: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: prod
        - namespaceSelector:
            matchLabels:
              name: stage
          podSelector:
            matchLabels:
              role: db-connect

        - podSelector:
            matchLabels:
              role: db-external-connect
          namespaceSelector: {}
```

```sh
k apply -f 23_allow.yaml
```

## 24

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k create ns app-system
k create deployment important-app --image nginx -o yaml --dry-run=client -n app-system >24.yaml
```

```yaml
# vim 24.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  creationTimestamp: null
  labels:
    app: important-app
  name: important-app
  namespace: app-system
spec:
  selector:
    matchLabels:
      app: important-app
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: important-app
    spec:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: "NoSchedule"
      containers:
      - image: nginx
        name: nginx
        resources: {}
```

```sh
k apply -f 24.yaml
k get no
k get po -n app-system -o wide
```

## 25

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k create ns app2-system
k create deployment important-app2 --image nginx --replicas 3 -n app2-system -o yaml --dry-run=client > 25.yaml
```

```sh
# vim 25.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: important-app2
  name: important-app2
  namespace: app2-system
spec:
  replicas: 3
  selector:
    matchLabels:
      app: important-app2
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: important-app2
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - important-app2
            topologyKey: "kubernetes.io/hostname"
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: "NoSchedule"
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}
```

```sh
k create poddisruptionbudget important-app2 -n app2-system  --min-available 1 --selector app=important-app2
```
