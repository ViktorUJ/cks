---
custom_edit_url: null
---

# 01

Solutions for CKAD Mock exam #01

[Video Solution](https://www.youtube.com/watch?v=yQK7Ca8d-yw)

## 01

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k get ns  apx-z993845
k create ns apx-z993845

k run webhttpd --image httpd:alpine -n apx-z993845
k get po -n  apx-z993845
```

## 02

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k create deployment nginx-app --image nginx:alpine-slim --replicas 2
k get deployment nginx-app
```

## 03

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k create ns dev-db
k create secret -n dev-db generic dbpassword --from-literal pwd=my-secret-pwd
k run db-pod --namespace dev-db --labels type=db --image mysql:8.0 --dry-run=client -o yaml >3.yaml
```

Edit definition file and add env variable to have

```yaml
#  vim  3.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    type: db
  name: db-pod
  namespace: dev-db
spec:
  containers:
  - image: mysql:8.0
    name: db-pod
    env:
    - name: MYSQL_ROOT_PASSWORD
      valueFrom:
        secretKeyRef:
          name: dbpassword
          key: pwd
```

Apply the changes

```sh
k  apply -f 3.yaml
```

## 04

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
$ k get po -n rsapp

NAME               READY   STATUS             RESTARTS   AGE
rs-app2223-78skl   0/1     ImagePullBackOff   0          7m55s
rs-app2223-wg4w7   0/1     ImagePullBackOff   0          7m55s
```

1. Edit replicaset executing the following command:

```sh
k edit rs -n rsapp rs-app2223
# Then change container image from rrredis:aline to redis:alpine
```

2. As it is replicaset we need to delete existing pods to allow ReplicaSet recreate them.

```sh
k  delete po -n rsapp -l app=rs-app2223
```

3. Ensure that new pods are running

```sh
k get po -n rsapp
```

## 05

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k create deployment -n messaging msg --image redis
k expose -n messaging deployment/msg --name msg-service  --target-port 6379 --type ClusterIP --port 6379
```

## 06

1. Get manifest of the existing pod

```sh
kubectl config use-context cluster1-admin@cluster1

k get pod text-printer -o yaml > 6.yaml
```

2. Change the value of env var from RED to GREEN

```sh
# vim 6.yaml
...
  env:
    - name: COLOR
      value: GREEN
...
```

3. Remove existing pod and create new one from updated manifest

```sh
k delete pod text-printer --force
k apply -f 6.yaml
```

# 07

```
kubectl config use-context cluster1-admin@cluster1
```

Generate manifest file via cli.

```sh
k run appsec-pod --image ubuntu:22.04 --dry-run=client -o yaml > 7.yaml
```

Edit manifest by adding security configurations.

```yaml
# vim 7.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: appsec-pod
  name: appsec-pod
spec:
  containers:
  - image: ubuntu:22.04
    name: appsec-pod
    args:
    - sleep
    - "4800"
    securityContext:
      capabilities:
        add: ["SYS_TIME"]
      runAsUser: 0
```

Apply updated changes.

```sh
k apply -f 7.yaml
```

## 08

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k logs pods/app-xyz3322
k logs pods/app-xyz3322 > /opt/logs/app-xyz123.log
```

## 09

```sh
kubectl config use-context cluster1-admin@cluster1
```

1. Add a taint to node and generate manifest for the pod

```sh
k taint node --help

k taint node -l work_type=redis  app_type=alpha:NoSchedule

k run alpha --image redis --dry-run=client -o yaml > 9.yaml
```

2. Add `tolerations` to pod

```yaml
# vim 9.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: alpha
  name: alpha
spec:
  containers:
  - image: redis
    name: alpha
  tolerations:
  - key: "app_type"
    operator: "Equal"
    value: "alpha"
    effect: "NoSchedule"
```

```sh
k apply -f 9.yaml
```

## 10

```sh
kubectl config use-context cluster1-admin@cluster1
```

1. Add a taint to controlplane and generate manifest for the pod

```sh
kubectl get no

kubectl label node ${put-controlplane-hostname} app_type=beta

kubectl create deployment beta-apps --image nginx --replicas 3 --dry-run=client -o yaml > 10.yaml
```

2. Modify manifest file and add NodeAffinity

```yaml
# vim 10.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: beta-apps
  name: beta-apps
spec:
  replicas: 3
  selector:
    matchLabels:
      app: beta-apps
  template:
    metadata:
      labels:
        app: beta-apps
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: app_type
                operator: In
                values:
                - beta
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: "NoSchedule"
      containers:
      - image: nginx
        name: nginx
```
or

```yaml
# vim 10.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: beta-apps
  name: beta-apps
spec:
  replicas: 3
  selector:
    matchLabels:
      app: beta-apps
  template:
    metadata:
      labels:
        app: beta-apps
    spec:
      nodeSelector:
        app_type: beta
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: "NoSchedule"
      containers:
      - image: nginx
        name: nginx

```

```sh
k apply -f 10.yaml
```

## 11

```sh
kubectl config use-context cluster1-admin@cluster1
```

```yaml
#vim 11.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: cat
  namespace: cat
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /cat
        pathType: Prefix
        backend:
          service:
            name: cat
            port:
              number: 80
```

```sh
k apply -f 11.yaml
```

## 12

```sh
kubectl config use-context cluster1-admin@cluster1

k create ns web-ns

k run nginx1233 --namespace web-ns --image nginx --dry-run=client -o yaml > 12.yaml
```

Edit manifest by configuring liveness probes

```sh
# vim 12.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: nginx1233
  name: nginx1233
  namespace: web-ns
spec:
  containers:
  - image: nginx
    name: nginx1233
    livenessProbe:
      exec:
        command:
        - ls
        - /var/www/html/
      initialDelaySeconds: 10
      periodSeconds: 60
```

```sh
k apply -f 12.yaml
```

## 13

```sh
kubectl config use-context cluster1-admin@cluster1
```

```yaml
# vim 13.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: hi-job
spec:
  template:
    spec:
      containers:
      - name: hi-job
        image: busybox
        command: ["echo", "hello world"]
      restartPolicy: Never
  backoffLimit: 6
  completions: 3
```

```sh
k apply -f 13.yaml
```

## 14

```yaml
kubectl config use-context cluster1-admin@cluster1
```

```sh
k run multi-pod --image nginx:alpine-slim --env type=alpha -o yaml --dry-run=client >14.yaml
```

```yaml
# vim 14.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: multi-pod
  name: multi-pod
spec:
  containers:
  - env:
    - name: type
      value: alpha
    image: nginx:alpine-slim
    name: alpha

  - env:
    - name: type
      value: beta
    image: busybox
    name: beta
    command: ["sleep","4800"]

    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

```sh
k apply -f 14.yaml
k get po multi-pod
```

## 15

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k get no -l node_name=node_2
# ssh to worker node
sudo mkdir /pv/analytics -p
sudo chmod  777 -R /pv/analytics
exit
```

```yaml
# vim 15.yaml
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-analytics
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/pv/analytics"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-analytics
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi
---
apiVersion: v1
kind: Pod
metadata:
  name: analytics
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: pvc-analytics
  nodeSelector:
    node_name: node_2
  containers:
    - name: task-pv-container
      image: busybox
      command: ["sleep","60000"]
      volumeMounts:
        - mountPath: "/pv/analytics"
          name: task-pv-storage
```

```sh
k apply -f 15.yaml
```

## 16

```sh
kubectl config use-context cluster1-admin@cluster1
```

[doc](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/)

```yaml
#vim 16.yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: operators.stable.example.com
spec:
  group: stable.example.com
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                name:
                  type: string
                email:
                  type: string
                age:
                  type: integer
  scope: Namespaced
  names:
    plural: operators
    singular: operator
    kind: Operator
    shortNames:
    - op
```

```sh
k apply -f 16.yaml
```

## 17

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
echo "kubectl top nodes" > /opt/18/nodes.txt

echo "kubectl top pod --all-namespaces --sort-by cpu" > /opt/18/pods.txt
```

## 18

```
kubectl config use-context cluster1-admin@cluster1
```

```sh
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update


helm install prom prometheus-community/kube-prometheus-stack \
  --namespace monitoring --create-namespace
```
