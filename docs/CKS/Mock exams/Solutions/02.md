# 02 - Solutions

Solutions for CKS Mock exam #02

## 01

```sh
kubectl config use-context cluster1-admin@cluster1
```

Go to the docs -> https://kubernetes.io/docs/home/
and find template for **RuntimeClass**

```yaml
# vim 1.yaml
# RuntimeClass is defined in the node.k8s.io API group
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  # The name the RuntimeClass will be referenced by.
  # RuntimeClass is a non-namespaced resource.
  name: gvisor
# The name of the corresponding CRI configuration
handler: runsc
```

```sh
k apply -f 1.yaml
k get runtimeclasses.node.k8s.io
```

```sh
k get no --show-labels
```

```sh
k label nodes {node2} RuntimeClass=runsc
```

```sh
k get deployment -n team-purple
k edit deployment -n team-purple
```

```yaml
....
        runtimeClassName:  gvisor                # add to all deployment
        nodeSelector:                            # add to all deployment
          RuntimeClass: runsc                    # add to all deployment
....
```

```sh
# ckeck pods in  ns  team-purple
k get po -n team-purple
```

```sh
mkdir -p /var/work/tests/artifacts/1/
```

```sh
k get po -n team-purple

k exec {pod1} -n team-purple -- dmesg

# find    Starting gVisor..

k exec {pod1} -n team-purple -- dmesg >/var/work/tests/artifacts/1/gvisor-dmesg
```

## 02

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k get po -n team-xxx -o yaml | grep 'image:' | uniq | grep -v 'docker'
```

```yaml
    - image: nginx:1.19-alpine-perl
      image: mariadb:10.8-focal
      image: mysql:8.0.33
    - image: nginx:1.23-bullseye-perl
```

```sh
# find all image with 'CRITICAL'
trivy image --severity CRITICAL  --quiet {image_name}
```

```sh
# trivy image --severity CRITICAL  --quiet nginx:1.19-alpine-perl

nginx:1.19-alpine-perl (alpine 3.13.5)

Total: 9 (CRITICAL: 9)
.......
```

```sh
$ trivy image --severity CRITICAL  --quiet mariadb:10.8-focal

mariadb:10.8-focal (ubuntu 20.04)

Total: 0 (CRITICAL: 0)

usr/local/bin/gosu (gobinary)

Total: 4 (CRITICAL: 4)
.......
```

```sh
$ trivy image --severity CRITICAL  --quiet mysql:8.0.33

mysql:8.0.33 (oracle 8.8)

Total: 0 (CRITICAL: 0)


usr/local/bin/gosu (gobinary)

Total: 3 (CRITICAL: 3)

.......
```

```sh
$ trivy image --severity CRITICAL  --quiet nginx:1.23-bullseye-perl



nginx:1.23-bullseye-perl (debian 11.7)

Total: 12 (CRITICAL: 12)
.......
```

**nginx:1.23-bullseye-perl** is the image with most **CRITICAL** vulnerabilities.

```sh
trivy image --format cyclonedx --output /var/work/02/critical_image.json  nginx:1.23-bullseye-perl
```

```sh
bom generate --image registry.k8s.io/kube-scheduler:v1.32.0 --format json --output /var/work/02/kube_scheduler_sbom.json
```

```sh
trivy sbom --format json --output /var/work/02/result_sbom.json /var/work/02/check_sbom.json
```

## 03

```sh
kubectl config use-context cluster2-admin@cluster2
```

https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/

```sh
k get no
ssh {control-plane}
```

```sh
sudo su

mkdir -p /etc/kubernetes/policy/
```

```yaml
# vim /etc/kubernetes/policy/log-policy.yaml
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
- level: Metadata
  resources:
  - group: "" # core API group
    resources: ["secrets"]
  namespaces: ["prod"]
- level: RequestResponse
  resources:
  - group: "" # core API group
    resources: ["configmaps"]
  namespaces: ["billing"]
- level: None
```

```yaml
# vim /etc/kubernetes/manifests/kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.2.16.248:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=10.2.16.248
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    - --audit-policy-file=/etc/kubernetes/policy/log-policy.yaml           # add
    - --audit-log-path=/var/logs/kubernetes-api.log                        # add

    image: registry.k8s.io/kube-apiserver:v1.28.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 10.2.16.248
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-apiserver
    readinessProbe:
      failureThreshold: 3
      httpGet:
        host: 10.2.16.248
        path: /readyz
        port: 6443
        scheme: HTTPS
      periodSeconds: 1
      timeoutSeconds: 15
    resources:
      requests:
        cpu: 250m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 10.2.16.248
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/ca-certificates
      name: etc-ca-certificates
      readOnly: true
    - mountPath: /etc/pki
      name: etc-pki
      readOnly: true
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readOnly: true
    - mountPath: /usr/share/ca-certificates
      name: usr-share-ca-certificates
      readOnly: true

    - mountPath: /etc/kubernetes/policy/log-policy.yaml    # add
      name: audit                                          # add
      readOnly: true                                       # add
    - mountPath: /var/logs/                                # add
      name: audit-log                                      # add
      readOnly: false                                      # add

  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  - hostPath:
      path: /etc/pki
      type: DirectoryOrCreate
    name: etc-pki
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /usr/local/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-local-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-share-ca-certificates

  - name: audit                                             # add
    hostPath:                                               # add
      path: /etc/kubernetes/policy/log-policy.yaml          # add
      type: File                                            # add
                                                            # add
  - name: audit-log                                         # add
    hostPath:                                               # add
      path: /var/logs/                                      # add
      type: DirectoryOrCreate                               # add

status: {}
```

```sh
service kubelet restart
k get no
k get secret -n prod
k get configmaps -n billing
```

```sh
$ cat /var/logs/kubernetes-api.log | jq  | grep secrets  -B 5 -A 5
--
  "kind": "Event",
  "apiVersion": "audit.k8s.io/v1",
  "level": "Metadata",
  "auditID": "a6b8945f-4914-4ba9-a80a-ea2441ad1e4f",
  "stage": "ResponseComplete",
  "requestURI": "/api/v1/namespaces/prod/secrets/db",
  "verb": "get",
  "user": {
    "username": "system:serviceaccount:prod:k8api",
    "uid": "cd47986d-8f88-4451-9de4-77fb3e9d46bb",
    "groups": [
--
  "sourceIPs": [
    "10.0.229.65"
  ],
  "userAgent": "curl/8.2.1",
  "objectRef": {
    "resource": "secrets",
    "namespace": "prod",
    "name": "db",
    "apiVersion": "v1"
  },
  "responseStatus": {
```

```sh
$ cat /var/logs/kubernetes-api.log | jq  | grep configmaps  -B 5 -A 5

--
  "sourceIPs": [
    "10.2.16.248"
  ],
  "userAgent": "kubectl/v1.28.0 (linux/arm64) kubernetes/855e7c4",
  "objectRef": {
    "resource": "configmaps",
    "namespace": "billing",
    "name": "bill",
    "apiVersion": "v1"
  },
  "requestReceivedTimestamp": "2023-09-27T19:14:33.778635Z",
--
  "kind": "Event",
  "apiVersion": "audit.k8s.io/v1",
  "level": "RequestResponse",
  "auditID": "0266674d-db53-4a3d-bf9c-940c6aa43440",
  "stage": "ResponseComplete",
  "requestURI": "/api/v1/namespaces/billing/configmaps/bill?fieldManager=kubectl-edit&fieldValidation=Strict",
  "verb": "patch",
  "user": {
    "username": "kubernetes-admin",
    "groups": [
      "system:masters",

```

## 04

```sh
kubectl config use-context cluster3-admin@cluster3
```

```sh
k get no
ssh {control-plane}
```

```sh
sudo su

kube-bench | grep '1.2.15' -A 5
# read   and fix
```

```sh
$ kube-bench | grep '1.2.15' -A 5

[FAIL] 1.2.15 Ensure that the --profiling argument is set to false (Automated)
[FAIL] 1.2.16 Ensure that the --audit-log-path argument is set (Automated)
[FAIL] 1.2.17 Ensure that the --audit-log-maxage argument is set to 30 or as appropriate (Automated)
[FAIL] 1.2.18 Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate (Automated)
[FAIL] 1.2.19 Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate (Automated)
[WARN] 1.2.20 Ensure that the --request-timeout argument is set as appropriate (Manual)
--
1.2.15 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml
on the control plane node and set the below parameter.
--profiling=false
```

```yaml
# vim  /etc/kubernetes/manifests/kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.10.1.23:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=10.10.1.23
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    - --profiling=false   # add it
........
```

```sh
service kubelet restart    # restart kubelet for faster apply changes
```

```sh
# check api status
kubectl get po -n kube-system
```

```text
calico-kube-controllers-7498b9bb4c-s4wl6   1/1     Running   0            80m
calico-node-x8fgk                          1/1     Running   0            80m
calico-node-x8sd6                          1/1     Running   0            80m
coredns-668d6bf9bc-6h94f                   1/1     Running   0            80m
coredns-668d6bf9bc-sgsjc                   1/1     Running   0            80m
etcd-ip-10-10-1-23                         1/1     Running   0            80m
kube-apiserver-ip-10-10-1-23               1/1     Running   0            2m41s
kube-controller-manager-ip-10-10-1-23      1/1     Running   1 (3m ago)   80m
kube-proxy-dszr5                           1/1     Running   0            80m
kube-proxy-tvdsp                           1/1     Running   0            80m
kube-scheduler-ip-10-10-1-23               1/1     Running   1 (3m ago)   80m
```

```sh
# check 1.2.15
kube-bench | grep '1.2.15' -A 5
```

```text
[PASS] 1.2.15 Ensure that the --profiling argument is set to false (Automated)
```

```sh
kube-bench | grep '1.3.2' -A 5
# read   and fix
```

```text
kube-bench | grep '1.3.2' -A 5
[FAIL] 1.3.2 Ensure that the --profiling argument is set to false (Automated)
[PASS] 1.3.3 Ensure that the --use-service-account-credentials argument is set to true (Automated)
[PASS] 1.3.4 Ensure that the --service-account-private-key-file argument is set as appropriate (Automated)
[PASS] 1.3.5 Ensure that the --root-ca-file argument is set as appropriate (Automated)
[PASS] 1.3.6 Ensure that the RotateKubeletServerCertificate argument is set to true (Automated)
[PASS] 1.3.7 Ensure that the --bind-address argument is set to 127.0.0.1 (Automated)
--
1.3.2 Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube-controller-manager.yaml
on the control plane node and set the below parameter.
--profiling=false

1.4.1 Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube-scheduler.yaml file
on the control plane node and set the below parameter.
```

```sh
kube-bench | grep '1.4.1' -A 5
# read   and fix
```

```sh
 kube-bench | grep '1.4.1' -A 5
[FAIL] 1.4.1 Ensure that the --profiling argument is set to false (Automated)
[PASS] 1.4.2 Ensure that the --bind-address argument is set to 127.0.0.1 (Automated)

== Remediations master ==
1.1.9 Run the below command (based on the file location on your system) on the control plane node.
For example, chmod 600 <path/to/cni/files>
--
1.4.1 Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube-scheduler.yaml file
on the control plane node and set the below parameter.
--profiling=false
```

Exit to work PC

```sh
k get no
ssh {work node}
```

```sh
sudo su

kube-bench | grep '4.2.6' -A 5
```

```text
[FAIL] 4.2.6 Ensure that the --protect-kernel-defaults argument is set to true (              Automated)


4.2.6 If using a Kubelet config file, edit the file to set protectKernelDefaults: true.
```

```sh
$ ps  aux | grep kubelet

root       37209  0.6  2.1 1968044 83328 ?       Ssl  19:31   0:01 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.10 --node-labels=node_name=node_1,work_type=worker
root       38427  0.0  0.0   6416  1792 pts/1    S+   19:35   0:00 grep --color=auto kubelet
```

```yaml
# vim /var/lib/kubelet/config.yaml
# add protectKernelDefaults: true
......
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s
protectKernelDefaults: true      # add it
```

```sh
service kubelet restart    # restart kubelet for faster apply changes
```

```sh
# check result
kube-bench | grep '4.2.6' -A 5
```

```text
[PASS] 4.2.6 Ensure that the --protect-kernel-defaults argument is set to true (Automated)
```

Exit to work PC

## 05

```sh
kubectl config use-context cluster6-admin@cluster6
```

```sh
k get secret db  -n team-5  -o yaml
```

```sh
apiVersion: v1
data:
  password: UGExNjM2d29yRA==
  user: YWQtYWRtaW4=
kind: Secret
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","data":{"password":"UGExNjM2d29yRA==","user":"YWQtYWRtaW4="},"kind":"Secret","metadata":{"annotations":{},"creationTimestamp":null,"name":"db","namespace":"team-5"}}
  creationTimestamp: "2023-09-27T16:47:13Z"
  name: db
  namespace: team-5
  resourceVersion: "540"
  uid: ba6e2888-6f02-4731-bba4-39df2fefc91d
type: Opaque
```

```sh
mkdir /var/work/tests/artifacts/5/ -p
echo {user} | base64 -d > /var/work/tests/artifacts/5/user
echo {password} | base64 -d > /var/work/tests/artifacts/5/password
```

```sh
k create  secret generic  db-admin -n team-5 --from-literal user=xxx --from-literal password=yyyy
k run db-admin --image viktoruj/cks-lab -n team-5 -o yaml  --dry-run=client  --command sleep 60000 >5.yaml
```

https://kubernetes.io/docs/concepts/configuration/secret/

```yaml
# vim 5.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: db-admin
  name: db-admin
  namespace: team-5
spec:
  volumes:
    - name: db-admin
      secret:
        secretName: db-admin
  containers:
  - command:
    - sleep
    - "60000"
    image: viktoruj/cks-lab
    name: db-admin
    volumeMounts:
      - name: db-admin
        readOnly: true
        mountPath: "/mnt/secret"
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

## 06

```sh
kubectl config use-context cluster4-admin@cluster4

```

```sh
k get po -n kube-system  | grep api
```

```sh
k exec  -n kube-system  kube-apiserver-ip-10-2-11-163  -- kube-apiserver --help  | grep cip
      --tls-cipher-suites strings              Comma-separated list of cipher suites for the server. If omitted, the default Go cipher suites will be used.
```

```sh
k exec  -n kube-system  kube-apiserver-ip-10-2-11-163  -- kube-apiserver --help  | grep tls | grep min
--tls-min-version string                 Minimum TLS version supported. Possible values: VersionTLS10, VersionTLS11, VersionTLS12, VersionTLS13

k get po -n kube-system |  grep etcd
k exec  -n kube-system etcd-ip-10-2-11-163 -- etcd --help | grep cip
--cipher-suites ''
    Comma-separated list of supported TLS cipher suites between client/server and peers (empty will be auto-populated by Go)
```

```sh
k get no

ssh {control-plane}
```

```sh
sudo su
```

```yaml
# vim /etc/kubernetes/manifests/etcd.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://10.2.11.163:2379
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384       # add
    - --advertise-client-urls=https://10.2.11.163:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --experimental-initial-corrupt-check=true
    - --experimental-watch-progress-notify-interval=5s
    - --initial-advertise-peer-urls=https://10.2.11.163:2380
    - --initial-cluster=ip-10-2-11-163=https://10.2.11.163:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --listen-client-urls=https://127.0.0.1:2379,https://10.2.11.163:2379
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --listen-peer-urls=https://10.2.11.163:2380
    - --name=ip-10-2-11-163
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
```

```yaml
# vim /etc/kubernetes/manifests/kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.2.11.163:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384        # add
    - --tls-min-version=VersionTLS13                                                                         # add
    - --advertise-address=10.2.11.163
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
..........
```

```sh
service kubelet restart
```

```sh
# wait to start etcd and api server
k get no
```

```sh
# to check  static pods use this command
crictl ps -a | grep kube-apiserver
crictl ps -a  | grep etcd
```

## 07

```sh
kubectl config use-context cluster5-admin@cluster5
```

https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/

```sh
k get no
ssh {control-plane}
```

```sh
sudo su
mkdir /etc/kubernetes/enc/  -p
```

```yaml
# vim /etc/kubernetes/enc/enc.yaml
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
      - secrets
    providers:

      - aescbc:
          keys:
            - name: key1
              secret: MTIzNDU2Nzg5MDEyMzQ1Ng==
      - identity: {}
```

```yaml
# vim /etc/kubernetes/manifests/kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.20.30.40:443
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    ...
    - --encryption-provider-config=/etc/kubernetes/enc/enc.yaml  # add this line
    volumeMounts:
    ...
    - name: enc                           # add this line
      mountPath: /etc/kubernetes/enc      # add this line
      readOnly: true                      # add this line
    ...
  volumes:
  ...
  - name: enc                             # add this line
    hostPath:                             # add this line
      path: /etc/kubernetes/enc           # add this line
      type: DirectoryOrCreate             # add this line
  ...
```

```sh
service kubelet restart
k get no
# wait k8s ready
```

```sh
k create secret generic test-secret -n prod --from-literal password=strongPassword
```

```sh
# encrypt all secrets in stage ns with new config
kubectl get secrets -n stage -o json | kubectl replace -f -
```

```sh
# check
ETCDCTL_API=3 etcdctl \
   --cacert=/etc/kubernetes/pki/etcd/ca.crt   \
   --cert=/etc/kubernetes/pki/etcd/server.crt \
   --key=/etc/kubernetes/pki/etcd/server.key  \
   get /registry/secrets/stage/stage | hexdump -C
```

exit to work pc

## 08

```sh
kubectl config use-context cluster6-admin@cluster6
```

https://kubernetes.io/docs/concepts/services-networking/network-policies/

```yaml
# vim 8_deny.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: prod-db

spec:
  podSelector: {}
  policyTypes:
  - Ingress
```

```sh
k apply -f 8_deny.yaml
```

```sh
k get ns --show-labels
```

```yaml
# vim 8_allow.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-policy
  namespace: prod-db
spec:
  podSelector:
    matchLabels: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: prod
        - namespaceSelector:
            matchLabels:
              name: stage
          podSelector:
            matchLabels:
              role: db-connect

        - podSelector:
            matchLabels:
              role: db-external-connect
          namespaceSelector: {}
```

```sh
k apply -f 8_allow.yaml
```

## 09

```sh
kubectl config use-context cluster6-admin@cluster6
```

```sh
cat /opt/course/9/profile
k get no
k label no {worker node} security=apparmor
```

```sh
scp /opt/course/9/profile {worker node}:/tmp/
ssh {worker node}
sudo su
```

```sh
apparmor_parser -q /tmp/profile
apparmor_status
apparmor_status | grep 'very-secure'
```

Exit to the work pc

```sh
mkdir /var/work/tests/artifacts/9/ -p
k create deployment apparmor -n apparmor --image nginx:1.19.2 --dry-run=client -o yaml >9.yaml
```

```yaml
# vim 9.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: apparmor
  name: apparmor
  namespace: apparmor
spec:
  replicas: 1
  selector:
    matchLabels:
      app: apparmor
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: apparmor
    spec:
      nodeSelector:                             # add it
        security: apparmor                      # add it
      securityContext:
        appArmorProfile:                        # add it
          type: Localhost                       # add it
          localhostProfile: very-secure         # add it
      containers:
      - image: nginx:1.19.2
        name: c1     # update
        resources: {}
status: {}
```

```sh
k apply -f 9.yaml
k get po -n apparmor
```

```text
NAME                        READY   STATUS             RESTARTS     AGE
apparmor-555d68c4d8-ntcgl   0/1     CrashLoopBackOff   1 (8s ago)   10s
```

```sh
k logs {apparmor-xxxx} -n apparmor
```

```text
/docker-entrypoint.sh: 13: /docker-entrypoint.sh: cannot create /dev/null: Permission denied
/docker-entrypoint.sh: No files found in /docker-entrypoint.d/, skipping configuration
2023/09/29 06:14:49 [emerg] 1#1: mkdir() "/var/cache/nginx/client_temp" failed (13: Permission denied)
nginx: [emerg] mkdir() "/var/cache/nginx/client_temp" failed (13: Permission denied)
```

```sh
k logs {apparmor-xxxx} -n apparmor>/var/work/tests/artifacts/9/log
```

## 10

```sh
kubectl config use-context cluster6-admin@cluster6
```

https://kubernetes.io/docs/tasks/configure-pod-container/security-context/

add readOnlyRootFilesystem  and volumes to write
```sh
$ k edit  deployment  secure  -n secure
```

Add line to container level

```yaml
securityContext: # add
   readOnlyRootFilesystem: true # add
   runAsGroup: 3000
   runAsUser: 3000
   allowPrivilegeEscalation: false
volumeMounts:  # to c1 container
          - mountPath: /tmp
            name: temp-vol

# add to spec level

volumes:
- emptyDir: {}
  name: temp-vol
```

Check pod to be running

```yaml
# k edit  deployment  secure  -n secure
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: secure
  name: secure
  namespace: secure
spec:
  replicas: 1
  selector:
    matchLabels:
      app: secure
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: secure
    spec:
      containers:
      - command:
        - sh
        - -c
        - while true ; do  echo "$(date) i am working . c1 . $(id)";  sleep 10  ;done
        image: viktoruj/cks-lab
        imagePullPolicy: Always
        name: c1
        resources: {}
        securityContext:
          readOnlyRootFilesystem: true
          runAsGroup: 3000
          runAsUser: 3000
          allowPrivilegeEscalation: false
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /tmp
          name: temp-vol
      - command:
        - sh
        - -c
        - while true ; do  echo "$(date) i am working . c2 . $(id)";  sleep 10  ;done
        image: viktoruj/cks-lab
        imagePullPolicy: Always
        name: c2
        resources: {}
        securityContext:
          readOnlyRootFilesystem: true
          runAsGroup: 3000
          runAsUser: 3000
          allowPrivilegeEscalation: false
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      - command:
        - sh
        - -c
        - while true ; do  echo "$(date) i am working . c3 . $(id)";  sleep 10  ;done
        image: viktoruj/cks-lab
        imagePullPolicy: Always
        name: c3
        resources: {}
        securityContext:
          readOnlyRootFilesystem: true
          runAsGroup: 3000
          runAsUser: 3000
          allowPrivilegeEscalation: false
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
      volumes:
      - emptyDir: {}
        name: temp-vol
```

```sh
k get po -n secure
```

```text
NAME                      READY   STATUS    RESTARTS   AGE
secure-69bf877687-79pn6   3/3     Running   0          67s
```

## 11

```sh
kubectl config use-context cluster6-admin@cluster6
```

```sh
k get sa dev  -n rbac-1
k get rolebindings.rbac.authorization.k8s.io  -n rbac-1 -o wide
```

```sh
k edit role dev   -n rbac-1
```

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: dev
  namespace: rbac-1
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - create
  - watch #  update form  delete
  - list
```

```sh
k create role dev -n rbac-2 --resource configmaps --verb get,list
k create rolebinding  dev -n rbac-2 --serviceaccount rbac-1:dev --role dev
k run  dev-rbac -n rbac-1 --image viktoruj/cks-lab -o yaml --dry-run=client --command sleep 60000   >11.yaml
```

https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/

```yaml
# vim 11.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: dev-rbac
  name: dev-rbac
  namespace: rbac-1
spec:
  serviceAccountName: dev              # add it
  containers:
  - command:
    - sleep
    - "60000"
    image: viktoruj/cks-lab
    name: dev-rbac
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

```sh
k apply -f 11.yaml
k get po -n rbac-1
```

## 12

```sh
kubectl config use-context cluster7-admin@cluster7
```

```sh
k get no
ssh {work node}
```

```sh
sudo su
```

run *falco* and check access to `/etc/shadow`

```sh
$ falco
Wed Feb 19 05:30:45 2025: Falco version: 0.40.0 (aarch64)
Wed Feb 19 05:30:45 2025: Falco initialized with configuration files:
Wed Feb 19 05:30:45 2025:    /etc/falco/config.d/engine-kind-falcoctl.yaml | schema validation: ok
Wed Feb 19 05:30:45 2025:    /etc/falco/falco.yaml | schema validation: ok
Wed Feb 19 05:30:45 2025: System info: Linux version 6.8.0-1021-aws (buildd@bos03-arm64-027) (aarch64-linux-gnu-gcc-12 (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0, GNU ld (GNU Binutils for Ubuntu) 2.38) #23~22.04.1-Ubuntu SMP Tue Dec 10 16:31:58 UTC 2024
Wed Feb 19 05:30:45 2025: Loading rules from:
Wed Feb 19 05:30:45 2025:    /etc/falco/falco_rules.yaml | schema validation: ok
Wed Feb 19 05:30:45 2025:    /etc/falco/falco_rules.local.yaml | schema validation: none
Wed Feb 19 05:30:45 2025: The chosen syscall buffer dimension is: 8388608 bytes (8 MBs)
Wed Feb 19 05:30:45 2025: Starting health webserver with threadiness 2, listening on 0.0.0.0:8765
Wed Feb 19 05:30:45 2025: Loaded event sources: syscall
Wed Feb 19 05:30:45 2025: Enabled event sources: syscall
Wed Feb 19 05:30:45 2025: Opening 'syscall' source with Kernel module
05:30:46.587810678: Warning Sensitive file opened for reading by non-trusted program (file=/etc/shadow gparent=systemd ggparent=<NA> gggparent=<NA> evt_type=openat user=root user_uid=0 user_loginuid=-1 process=app1 proc_exepath=/app1 parent=containerd-shim command=app1 terminal=0 container_id=bf00df0b23b0 container_name=deployment)
05:30:48.588959184: Warning Sensitive file opened for reading by non-trusted program (file=/etc/shadow gparent=systemd ggparent=<NA> gggparent=<NA> evt_type=openat user=root user_uid=0 user_loginuid=-1 process=app1 proc_exepath=/app1 parent=containerd-shim command=app1 terminal=0 container_id=bf00df0b23b0 container_name=deployment)
```

find (grep) needed fields from falco outputL

```sh
# falco --list | grep  k8s
.......
k8s.ns.name                   The Kubernetes namespace name. This field is extracted from the container runtime socket
k8s.pod.name                  The Kubernetes pod name. This field is extracted from the container runtime socket
k8s.pod.id                    [LEGACY] The Kubernetes pod UID, e.g. 3e41dc6b-08a8-44db-bc2a-3724b18ab19a. This legacy
                              field points to `k8s.pod.uid`; however, the pod ID typically refers to the pod sandbox
                              ID. We recommend using the semantically more accurate `k8s.pod.uid` field. This field is
k8s.pod.uid                   The Kubernetes pod UID, e.g. 3e41dc6b-08a8-44db-bc2a-3724b18ab19a. Note that the pod UID
k8s.pod.sandbox_id            The truncated Kubernetes pod sandbox ID (first 12 characters), e.g 63060edc2d3a. The
                              `container.id` matches `k8s.pod.sandbox_id`, lacking other 'container.*' details.
k8s.pod.full_sandbox_id       The full Kubernetes pod / sandbox ID, e.g
k8s.pod.label                 (ARG_REQUIRED) The Kubernetes pod label. The label can be accessed either with the
........
```

We need  `k8s.ns.name` and `k8s.pod.name`.

```sh
# falco --list | grep  user
.......
user.uid                      user ID.
user.name                     user name.
user.homedir                  home directory of the user.
user.shell                    user's shell.
user.loginuid                 audit user id (auid), internally the loginuid is of type `uint32_t`.
.......
```

We need `user.name`

```sh
# falco --list | grep  container
.......

container.image               The container image name (e.g. falcosecurity/falco:latest for docker). In instances of
                              userspace container engine lookup delays, this field may not be available yet.
container.image.id            The container image id (e.g. 6f7e2741b66b). In instances of userspace container engine
container.type                The container type, e.g. docker, cri-o, containerd etc.
container.privileged          'true' for containers running as privileged, 'false' otherwise. In instances of userspace

.......
```

We need `container.image`

```yaml
#  vim /etc/falco/falco_rules.yaml

#   type  "/"   and find "Sensitive file opened for reading by non-trusted program"

# change output for the rule to

read %fd.name ns=%k8s.ns.name pod_name=%k8s.pod.name user_name=%user.name container_image=%container.image
```

run falco with the new rule , with  `-U/--unbuffered` to speed up  , and filter by the rule `/etc/shadow`

```sh
# falco -U 2>&1 | grep '/etc/shadow'
# wait 20 sec
06:06:05.515798114: Warning read /etc/shadow ns=default pod_name=deployment1-6bd8869f9f-fcmpx user_name=root container_image=docker.io/viktoruj/cks-lab:cks_mock2_12_app1
06:06:07.516948940: Warning read /etc/shadow ns=default pod_name=deployment1-6bd8869f9f-fcmpx user_name=root container_image=docker.io/viktoruj/cks-lab:cks_mock2_12_app1
........
```

Copy & paste the output into file `/tmp/log` and *exit* to *work pc*

```sh
# copy log to worker pc
mkdir -p /var/work/tests/artifacts/12/
scp {work node }:/tmp/log /var/work/tests/artifacts/12/
```

```sh
# scale down deployment1 to 0 replicas in namespace default
k scale deployment deployment1 --replicas=0 -n default
```

## 13

```sh
kubectl config use-context cluster8-admin@cluster8
```

```sh
k get no
ssh {control-plane}
```

```sh
# check  admission_config.json
cat  /etc/kubernetes/pki/admission_config.json
```

```sh
# check  admission_kube_config.yaml
cat  /etc/kubernetes/pki/webhook/admission_kube_config.yaml
```

```yaml
# vim  /etc/kubernetes/pki/webhook/admission_kube_config.yaml
apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority: /etc/kubernetes/pki/webhook/server.crt
    server: https://image-bouncer-webhook:30020/image_policy  # add
  name: bouncer_webhook
contexts:
- context:
    cluster: bouncer_webhook
    user: api-server
  name: bouncer_validator
current-context: bouncer_validator
preferences: {}
users:
- name: api-server
  user:
    client-certificate: /etc/kubernetes/pki/apiserver.crt
    client-key:  /etc/kubernetes/pki/apiserver.key

```

```yaml
# vim /etc/kubernetes/manifests/kube-apiserver.yaml
# add to api parametrs

- --enable-admission-plugins=NodeRestriction,ImagePolicyWebhook
- --admission-control-config-file=/etc/kubernetes/pki/admission_config.json
```

```sh
service kubelet restart
# exit to work pc
```

```sh
k run test-tag --image nginx
```

```text
Error from server (Forbidden): pods "test-tag" is forbidden: image policy webhook backend denied one or more images: Images using latest tag are not allowed
```

```sh
k run test-tag --image nginx:alpine3.17
k get po test-tag
```

```text
NAME       READY   STATUS    RESTARTS   AGE
test-tag   1/1     Running   0          4m47s
```

## 14

```Dockerfile
# vim  /var/work/14/Dockerfile
FROM ubuntu:20.04
RUN apt-get update
RUN apt-get -y install curl
RUN groupadd myuser
RUN useradd  -g myuser  myuser
USER myuser
CMD ["sh", "-c", "while true ; do  id ;  sleep 1  ;done"]
```

```sh
podman build -t cks:14 .

podman  run -d --name  cks-14   cks:14
sleep 2
podman logs cks-14 | grep myuser
```

```sh
podman stop cks-14
podman rm  cks-14
```

## 15

```sh
kubectl config use-context    cluster6-admin@cluster6
```

https://kubernetes.io/docs/tutorials/security/ns-level-pss/

```sh
k get ns team-red --show-labels

kubectl label --overwrite ns team-red  pod-security.kubernetes.io/enforce=baseline

k get ns team-red --show-labels
```

```sh
k get po -n team-red
# delete all pods in ns team-red

k delete po {pod_names} -n  team-red --force
```

```sh
k get po -n team-red

# No resources found in team-red namespace.
```

```sh
k  events  replicasets.apps  -n team-red
mkdir /var/work/tests/artifacts/15  -p
k  events  replicasets.apps  -n team-red >/var/work/tests/artifacts/15/logs
```

## 16

```sh
kubectl config use-context   cluster11-admin@cluster11
```

To retrieve a list of all endpoints managed by Cilium, the Cilium Endpoint (or cep) resource can be used:

```sh
kubectl get cep --all-namespaces
```

allow access to   **/public** path

```yaml
# vim 16_public.yaml

apiVersion: "cilium.io/v2"
kind: CiliumNetworkPolicy
metadata:
  name: "public"
  namespace: "production"
spec:
  endpointSelector:
    matchLabels:
      app: app
  ingress:
  - fromEntities:
     - cluster
    toPorts:
    - ports:
      - port: '80'
        protocol: TCP
      rules:
        http:
          - path: "/public.*"
```

```sh
k apply -f 16_public.yaml
```

allow access form  **finance** NS to **/private** path

```yaml
# vim 16_private.yaml
---
apiVersion: "cilium.io/v2"
kind: CiliumNetworkPolicy
metadata:
  name: "private"
  namespace: "production"
spec:
  endpointSelector:
    matchLabels:
      app: app
  ingress:
  - fromEndpoints:
    - matchLabels:
        k8s:io.kubernetes.pod.namespace: finance
    toPorts:
    - ports:
      - port: '80'
        protocol: TCP
      rules:
        http:
          - path: "/private.*"
```

```sh
k apply -f 16_private.yaml
```

**private** api from **finance** namespace

```sh
k exec  -n finance finance  --  curl http://portal.production/private/api123 --connect-timeout 1

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Server Name: ping_pong_server
URL: http://portal.production/private/api123
Client IP: 10.0.1.252
Method: GET
Protocol: HTTP/1.1
Headers:
X-Envoy-Internal: true
X-Request-Id: 5b49ea5f-abc6-4337-bdb2-8a8f932ca5ee
X-Envoy-Expected-Rq-Timeout-Ms: 3600000
User-Agent: curl/8.5.0
Accept: */*
X-Forwarded-Proto: http
100   310  100   310    0     0  78800      0 --:--:-- --:--:-- --:--:--  100k
```

**public** api from **finance** namespace

```sh
k exec  -n finance finance  --  curl http://portal.production/public/api123  --connect-timeout 1

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Server Name: ping_pong_server
URL: http://portal.production/public/api123
Client IP: 10.0.1.252
Method: GET
Protocol: HTTP/1.1
Headers:
Accept: */*
X-Forwarded-Proto: http
X-Envoy-Internal: true
X-Request-Id: bc4e8755-9d03-4299-8539-10f1bb32793a
X-Envoy-Expected-Rq-Timeout-Ms: 3600000
User-Agent: curl/8.5.0
100   309  100   309    0     0  73958      0 --:--:-- --:--:-- --:--:--  100k
```

**private** api from **external** namespace

```sh
k exec  -n external external  --  curl http://portal.production/private/api123 --connect-timeout 1

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Access denied
100    15  100    15    0     0   3069      0 --:--:-- --:--:-- --:--:--  3750
```

**public** api from **external** namespace

```sh
k exec  -n external external  --  curl http://portal.production/public/api123  --connect-timeout 1

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Server Name: ping_pong_server
URL: http://portal.production/public/api123
Client IP: 10.0.1.252
Method: GET
Protocol: HTTP/1.1
Headers:
User-Agent: curl/8.5.0
Accept: */*
X-Forwarded-Proto: http
X-Envoy-Internal: true
X-Request-Id: 7e766755-1322-4ef5-b190-3ffde601492d
X-Envoy-Expected-Rq-Timeout-Ms: 3600000
100   309  100   309    0     0  76655      0 --:--:-- --:--:-- --:--:--  100k
```

## 17

```sh
kubectl config use-context cluster9-admin@cluster9
```

```sh
k get crd
k get constraint
k get constrainttemplates
k edit constrainttemplates k8strustedimages
```

```sh
.......
  - rego: |
      package k8strustedimages

      violation[{"msg": msg}] {
       not images
       msg := "not trusted image!"
      }

      images {
        image := input.review.object.spec.containers[_].image
        not startswith(image, "docker-fake.io/")
        not startswith(image, "google-gcr-fake.com/")
        not startswith(image, "very-bad-registry.com/")  # add

      }
...........
```

```sh
k run test --image=nginx/very-bad-registry.com/test
```

## 18

```sh
kubectl config use-context cluster10-admin@cluster10
```

https://kubernetes.io/docs/tutorials/security/seccomp/

```sh
k get no
ssh {work node}
```

```sh
sudo su

mkdir /var/lib/kubelet/seccomp -p
cp /var/work/profile-nginx.json /var/lib/kubelet/seccomp/

# exit to work pc
```

```sh
k run seccomp --image nginx -o yaml --dry-run=client > 18.yaml
```

```yaml
# vim 18.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: seccomp
  name: seccomp
spec:
  securityContext:                          # add
    seccompProfile:                         # add
      type: Localhost                       # add
      localhostProfile: profile-nginx.json  # add
  containers:
  - image: nginx
    name: seccomp
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

```sh
k apply -f 18.yaml
k get po seccomp

```

## 19

```sh
kubectl config use-context cluster6-admin@cluster6
```

```sh
k create secret tls cks-local-tls  --cert=/var/work/19/cks.local.crt  --key=/var/work/19/cks.local.key -n team-19
````

```sh
k edit ing -n team-19
```

```yaml
.....

spec:
  ingressClassName: nginx
  tls:                                                # add it
  - hosts:                                            # add it
    - cks.local                                       # add it
    secretName: cks-local-tls                         # add it

.....
```

check certificate

```sh 
curl https://cks.local:31139 -kv
```

```text
.....
* Server certificate:
*  subject: C=US; ST=State; L=City; O=Organization; CN=cks.local
*  start date: Feb  5 05:42:59 2025 GMT
*  expire date: Feb  5 05:42:59 2026 GMT
*  issuer: C=US; ST=State; L=City; O=Organization; CN=cks.local
*  SSL certificate verify result: self-signed certificate (18), continuing anyway.
.....
```

## 20

[doc](https://kubernetes.io/docs/concepts/storage/projected-volumes/)

```sh
kubectl config use-context cluster6-admin@cluster6
```

```sh 
k create sa team20 -n team-20
```

```yaml
# k edit  sa team20  -n team-20 -o yaml 
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: false                       # add it   
metadata:
  creationTimestamp: "2025-03-19T05:20:26Z"
  name: team20
  namespace: team-20
  resourceVersion: "6506"
  uid: 74ce6f92-eb28-461b-9f81-22aa94918f2f
~                                                  
```

```yaml
# k edit deployment team20 -n team-20

apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "4"
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"creationTimestamp":null,"labels":{"app":"team20"},"name":"team20","namespace":"team-20"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"team20"}},"strategy":{},"template":{"metadata":{"creationTimestamp":null,"labels":{"app":"team20"}},"spec":{"containers":[{"image":"viktoruj/ping_pong:alpine","name":"app","ports":[{"containerPort":8080}]}]}}}}
  creationTimestamp: "2025-03-19T05:07:37Z"
  generation: 4
  labels:
    app: team20
  name: team20
  namespace: team-20
  resourceVersion: "4569"
  uid: b2f32d62-7fe9-4a00-999d-4fa4ab294874
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: team20
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: team20
    spec:
      containers:
      - image: viktoruj/ping_pong:alpine
        imagePullPolicy: IfNotPresent
        name: app
        ports:
        - containerPort: 8080
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:                                        # add it
        - mountPath: /var/team20/secret                      # add it
          name: team20-token                                 # add it
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: team20
      serviceAccountName: team20
      terminationGracePeriodSeconds: 30
      volumes:                                               # add it
      - name: team20-token                                   # add it
        projected:                                           # add it
          defaultMode: 420                                   # add it
          sources:                                           # add it
          - serviceAccountToken:                             # add it
              path: token                                    # add it
```

```sh
$ k exec  -ti -n team-20 team20-xxxx-- cat /var/team20/secret/token

eyJhbGciOiJSUzI1NiIsImtpZCI6ImdwVWN1d2NraG5VVWs5dlUzUm11cEhnTnhOa19yeTB3QXh0ZTdjZWNSRjgifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzQyMzY2MzczLCJpYXQiOjE3NDIzNjI3NzMsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwianRpIjoiYzZlYzZhMjItZjU0NS00NWYyLWIxY2EtYWU1NGMyNDZmMjU5Iiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJ0ZWFtLTIwIiwibm9kZSI6eyJuYW1lIjoiaXAtMTAtMTAtMS0zMCIsInVpZCI6ImJlNjU4NzQwLWViZWMtNDFjMi1iZDdmLTMyYTg5NTI1NDkyMyJ9LCJwb2QiOnsibmFtZSI6InRlYW0yMC01ZGY5NDRmNzY4LXN0dnc5IiwidWlkIjoiNDNjN2FjOWEtMjliNy00ZDk2LTlmOTgtYzQzYTU4ZjdiZTBjIn0sInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJ0ZWFtMjAiLCJ1aWQiOiI3NGNlNmY5Mi1lYjI4LTQ2MWItOWY4MS0yMmFhOTQ5MThmMmYifX0sIm5iZiI6MTc0MjM2Mjc3Mywic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OnRlYW0tMjA6dGVhbTIwIn0.G6NYi08uiBghWIR3CGKsIEzNgS_gnQo_QCCXscuEr9jhJxToLJbbZVcsCkqVC3oxWm6LoRxbO7vtAqfW2ytl0oyMkMLk_f3jWBsAArZXgn6_8uyiDv__tpRFmctRLjjlpo46yiOUW7CH18hYq7YC8BrHVibB4mnY1JqkoL2rvffoDPhhtGNXsehgetPXCO_nX3GuiniY4KCwj4jP2WZyf5mBzdzgtOyiioBMH-hF1soDWWY_gcgk3qHYvnr_ehIIADcvLsRSvz1oTlv4AUYvWsEDoiIUtBwfHGcyZMdsuRwoV_7mSgJ6vbPYHZCPA_lo2mxX9mn7aA2INDbN0t9a3g
```
