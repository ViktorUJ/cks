# 01

Solutions for CKS Mock exam #01

## 01

```sh
kubectl config use-context cluster1-admin@cluster1
```

-> https://kubernetes.io/docs/home/
and find  template for **RuntimeClass**

```yaml
# vim 1.yaml
# RuntimeClass is defined in the node.k8s.io API group
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  # The name the RuntimeClass will be referenced by.
  # RuntimeClass is a non-namespaced resource.
  name: gvisor
# The name of the corresponding CRI configuration
handler: runsc
```

```sh
k apply -f 1.yaml
k get runtimeclasses.node.k8s.io
```

```sh
k get no --show-labels
```

```sh
k label nodes {node2} RuntimeClass=runsc
```

```sh
k get deployment -n team-purple
k edit deployment -n team-purple
```

```yaml
        runtimeClassName:  gvisor                # add to all deployment
        nodeSelector:                            # add to all deployment
          RuntimeClass: runsc                    # add to all deployment
```

```sh
# ckeck pods in  ns  team-purple
k get po -n team-purple
```

```sh
mkdir -p /var/work/tests/artifacts/1/
```

```sh
k get po -n team-purple

k exec {pod1} -n team-purple -- dmesg

# find    Starting gVisor..

k exec {pod1} -n team-purple -- dmesg >/var/work/tests/artifacts/1/gvisor-dmesg
```

## 02

```sh
kubectl config use-context cluster1-admin@cluster1
```

```sh
k get po -n team-xxx -o yaml | grep 'image:' | uniq | grep -v 'docker'
```

```sh
k get no
ssh {node 2 }
```

```sh
# find all image with 'CRITICAL'
trivy  i {image} | grep 'CRITICAL'
```

```
# exit to worker PC
exit
```

```sh
k get deployment  -n team-xxx

k get deployment {deployment1}  -n team-xxx -o yaml | grep 'image:'

# if deployment has CRITICAL image
#   k scale   deployment {deployment_name}  -n team-xxx  --replicas 0
```

## 03

```sh
kubectl config use-context cluster2-admin@cluster2
```

https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/

```sh
k get no
ssh {control-plane}
```

```sh
sudo su

mkdir -p /etc/kubernetes/policy/
```

```sh
# vim /etc/kubernetes/policy/log-policy.yaml

apiVersion: audit.k8s.io/v1
kind: Policy
rules:
- level: Metadata
  resources:
  - group: "" # core API group
    resources: ["secrets"]
  namespaces: ["prod"]
- level: RequestResponse
  resources:
  - group: "" # core API group
    resources: ["configmaps"]
  namespaces: ["billing"]
- level: None
```

```yaml
# vim /etc/kubernetes/manifests/kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.2.16.248:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=10.2.16.248
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    - --audit-policy-file=/etc/kubernetes/policy/log-policy.yaml           # add
    - --audit-log-path=/var/logs/kubernetes-api.log                        # add

    image: registry.k8s.io/kube-apiserver:v1.28.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 10.2.16.248
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-apiserver
    readinessProbe:
      failureThreshold: 3
      httpGet:
        host: 10.2.16.248
        path: /readyz
        port: 6443
        scheme: HTTPS
      periodSeconds: 1
      timeoutSeconds: 15
    resources:
      requests:
        cpu: 250m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 10.2.16.248
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/ca-certificates
      name: etc-ca-certificates
      readOnly: true
    - mountPath: /etc/pki
      name: etc-pki
      readOnly: true
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readOnly: true
    - mountPath: /usr/share/ca-certificates
      name: usr-share-ca-certificates
      readOnly: true

    - mountPath: /etc/kubernetes/policy/log-policy.yaml    # add
      name: audit                                          # add
      readOnly: true                                       # add
    - mountPath: /var/logs/                                # add
      name: audit-log                                      # add
      readOnly: false                                      # add

  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  - hostPath:
      path: /etc/pki
      type: DirectoryOrCreate
    name: etc-pki
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /usr/local/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-local-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-share-ca-certificates

  - name: audit                                             # add
    hostPath:                                               # add
      path: /etc/kubernetes/policy/log-policy.yaml          # add
      type: File                                            # add
                                                            # add
  - name: audit-log                                         # add
    hostPath:                                               # add
      path: /var/logs/                                      # add
      type: DirectoryOrCreate                               # add

```

```sh
service kubelet restart
k get no
k get secret -n prod
k get configmaps -n billing

```

```json
# cat /var/logs/kubernetes-api.log | jq  | grep secrets  -B 5 -A 5

--
  "kind": "Event",
  "apiVersion": "audit.k8s.io/v1",
  "level": "Metadata",
  "auditID": "a6b8945f-4914-4ba9-a80a-ea2441ad1e4f",
  "stage": "ResponseComplete",
  "requestURI": "/api/v1/namespaces/prod/secrets/db",
  "verb": "get",
  "user": {
    "username": "system:serviceaccount:prod:k8api",
    "uid": "cd47986d-8f88-4451-9de4-77fb3e9d46bb",
    "groups": [
--
  "sourceIPs": [
    "10.0.229.65"
  ],
  "userAgent": "curl/8.2.1",
  "objectRef": {
    "resource": "secrets",
    "namespace": "prod",
    "name": "db",
    "apiVersion": "v1"
  },
  "responseStatus": {
```

```yaml
# cat /var/logs/kubernetes-api.log | jq  | grep configmaps  -B 5 -A 5
--
  "sourceIPs": [
    "10.2.16.248"
  ],
  "userAgent": "kubectl/v1.28.0 (linux/arm64) kubernetes/855e7c4",
  "objectRef": {
    "resource": "configmaps",
    "namespace": "billing",
    "name": "bill",
    "apiVersion": "v1"
  },
  "requestReceivedTimestamp": "2023-09-27T19:14:33.778635Z",
--
  "kind": "Event",
  "apiVersion": "audit.k8s.io/v1",
  "level": "RequestResponse",
  "auditID": "0266674d-db53-4a3d-bf9c-940c6aa43440",
  "stage": "ResponseComplete",
  "requestURI": "/api/v1/namespaces/billing/configmaps/bill?fieldManager=kubectl-edit&fieldValidation=Strict",
  "verb": "patch",
  "user": {
    "username": "kubernetes-admin",
    "groups": [
      "system:masters",

```

## 04

```sh
kubectl config use-context cluster3-admin@cluster3
```

```sh
k get no
ssh {control-plane}
```

```sh
sudo su

kube-bench | grep '1.2.16' -A 5
# read   and fix
```

```sh
kube-bench | grep '1.2.16' -A 5
[FAIL] 1.2.17 Ensure that the --profiling argument is set to false (Automated)
[FAIL] 1.2.18 Ensure that the --audit-log-path argument is set (Automated)
[FAIL] 1.2.19 Ensure that the --audit-log-maxage argument is set to 30 or as appropriate (Automated)
[FAIL] 1.2.20 Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate (Automated)
[FAIL] 1.2.21 Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate (Automated)
[WARN] 1.2.22 Ensure that the --request-timeout argument is set as appropriate (Manual)
--
1.2.16 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml
on the control plane node and set the below parameter.
--profiling=false
```

```sh
kube-bench | grep '1.3.2' -A 5
# read   and fix
```

```sh
kube-bench | grep '1.3.2' -A 5
[FAIL] 1.3.2 Ensure that the --profiling argument is set to false (Automated)
[PASS] 1.3.3 Ensure that the --use-service-account-credentials argument is set to true (Automated)
[PASS] 1.3.4 Ensure that the --service-account-private-key-file argument is set as appropriate (Automated)
[PASS] 1.3.5 Ensure that the --root-ca-file argument is set as appropriate (Automated)
[PASS] 1.3.6 Ensure that the RotateKubeletServerCertificate argument is set to true (Automated)
[PASS] 1.3.7 Ensure that the --bind-address argument is set to 127.0.0.1 (Automated)
--
1.3.2 Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube-controller-manager.yaml
on the control plane node and set the below parameter.
--profiling=false

1.4.1 Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube-scheduler.yaml file
on the control plane node and set the below parameter.
```

```sh
kube-bench | grep '1.4.1' -A 5
# read   and fix
```

```sh
 kube-bench | grep '1.4.1' -A 5
[FAIL] 1.4.1 Ensure that the --profiling argument is set to false (Automated)
[PASS] 1.4.2 Ensure that the --bind-address argument is set to 127.0.0.1 (Automated)

== Remediations master ==
1.1.9 Run the below command (based on the file location on your system) on the control plane node.
For example, chmod 600 <path/to/cni/files>
--
1.4.1 Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube-scheduler.yaml file
on the control plane node and set the below parameter.
--profiling=false


```

Exit to work PC

```sh
k get no
ssh {work node}
```

```sh
sudo su

kube-bench | grep '4.2.6' -A 5
# read and fix

# exit to work PC
```

## 05

```sh
kubectl config use-context cluster6-admin@cluster6
```

```sh
k get secret db  -n team-5  -o yaml
```

```yaml
apiVersion: v1
data:
  password: UGExNjM2d29yRA==
  user: YWQtYWRtaW4=
kind: Secret
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","data":{"password":"UGExNjM2d29yRA==","user":"YWQtYWRtaW4="},"kind":"Secret","metadata":{"annotations":{},"creationTimestamp":null,"name":"db","namespace":"team-5"}}
  creationTimestamp: "2023-09-27T16:47:13Z"
  name: db
  namespace: team-5
  resourceVersion: "540"
  uid: ba6e2888-6f02-4731-bba4-39df2fefc91d
type: Opaque

```

```sh
mkdir /var/work/tests/artifacts/5/ -p
echo {user} | base64 -d > /var/work/tests/artifacts/5/user
echo {password} | base64 -d > /var/work/tests/artifacts/5/password
```

```sh
k create  secret generic  db-admin -n team-5 --from-literal user=xxx --from-literal password=yyyy
k run db-admin --image viktoruj/cks-lab -n team-5 -o yaml  --dry-run=client  --command sleep 60000 >5.yaml
```

https://kubernetes.io/docs/concepts/configuration/secret/

```yaml
# vim 5.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: db-admin
  name: db-admin
  namespace: team-5
spec:
  volumes:
    - name: db-admin
      secret:
        secretName: db-admin
  containers:
  - command:
    - sleep
    - "60000"
    image: viktoruj/cks-lab
    name: db-admin
    volumeMounts:
      - name: db-admin
        readOnly: true
        mountPath: "/mnt/secret"
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

```

## 06

```sh
kubectl config use-context cluster4-admin@cluster4
```

```sh
k get po -n kube-system  | grep api
```

```sh
k exec  -n kube-system  kube-apiserver-ip-10-2-11-163  -- kube-apiserver --help  | grep cip
      --tls-cipher-suites strings              Comma-separated list of cipher suites for the server. If omitted, the default Go cipher suites will be used.
```

```sh
k exec  -n kube-system  kube-apiserver-ip-10-2-11-163  -- kube-apiserver --help  | grep tls | grep min
--tls-min-version string                 Minimum TLS version supported. Possible values: VersionTLS10, VersionTLS11, VersionTLS12, VersionTLS13

k get po -n kube-system |  grep etcd
k exec  -n kube-system etcd-ip-10-2-11-163 -- etcd --help | grep cip
--cipher-suites ''
    Comma-separated list of supported TLS cipher suites between client/server and peers (empty will be auto-populated by Go)

```

```sh
k get no

ssh {control-plane}
```

```sh
sudo su
vim /etc/kubernetes/manifests/kube-apiserver.yaml
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.2.11.163:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384  # add
    - --tls-min-version=VersionTLS13    # add
    - --advertise-address=10.2.11.163
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
..........
```

```yaml
# vim /etc/kubernetes/manifests/etcd.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://10.2.11.163:2379
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 # add
    - --advertise-client-urls=https://10.2.11.163:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --experimental-initial-corrupt-check=true
    - --experimental-watch-progress-notify-interval=5s
    - --initial-advertise-peer-urls=https://10.2.11.163:2380
    - --initial-cluster=ip-10-2-11-163=https://10.2.11.163:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --listen-client-urls=https://127.0.0.1:2379,https://10.2.11.163:2379
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --listen-peer-urls=https://10.2.11.163:2380
    - --name=ip-10-2-11-163
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
```

## 07

```
kubectl config use-context cluster5-admin@cluster5
```

https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/

```sh
k get no
ssh {control-plane}
```

```sh
sudo su
mkdir /etc/kubernetes/enc/  -p
```

```yaml
# vim /etc/kubernetes/enc/enc.yaml
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: MTIzNDU2Nzg5MDEyMzQ1Ng==
      - identity: {}
```

```yaml
# vim /etc/kubernetes/manifests/kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.20.30.40:443
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    ...
    - --encryption-provider-config=/etc/kubernetes/enc/enc.yaml  # add this line
    volumeMounts:
    ...
    - name: enc                           # add this line
      mountPath: /etc/kubernetes/enc      # add this line
      readOnly: true                      # add this line
    ...
  volumes:
  ...
  - name: enc                             # add this line
    hostPath:                             # add this line
      path: /etc/kubernetes/enc           # add this line
      type: DirectoryOrCreate             # add this line
  ...
```

```sh
service kubelet restart
k get no
# wait k8s ready
```

```sh
k create secret generic test-secret -n prod --from-literal password=strongPassword
```

```sh
# encrypt all secrets in stage ns with new config
kubectl get secrets -n stage -o json | kubectl replace -f -
```

```sh
# check
ETCDCTL_API=3 etcdctl \
   --cacert=/etc/kubernetes/pki/etcd/ca.crt   \
   --cert=/etc/kubernetes/pki/etcd/server.crt \
   --key=/etc/kubernetes/pki/etcd/server.key  \
   get /registry/secrets/stage/stage | hexdump -C

```

```sh
# exit  to work pc
```

## 08

```sh
kubectl config use-context cluster6-admin@cluster6
```

https://kubernetes.io/docs/concepts/services-networking/network-policies/

```yaml
# vim 8_deny.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: prod-db

spec:
  podSelector: {}
  policyTypes:
  - Ingress
```

```sh
k apply -f 8_deny.yaml
```

```sh
k get ns --show-labels
````

```yaml
# vim 8_allow.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-policy
  namespace: prod-db
spec:
  podSelector:
    matchLabels: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: prod
        - namespaceSelector:
            matchLabels:
              name: stage
          podSelector:
            matchLabels:
              role: db-connect

        - podSelector:
            matchLabels:
              role: db-external-connect
          namespaceSelector: {}
```

```sh
k apply -f 8_allow.yaml
```

## 09

```sh
kubectl config use-context cluster6-admin@cluster6
```

```sh
cat /opt/course/9/profile
k get no
k label no {worker node} security=apparmor
```

```sh
scp /opt/course/9/profile {worker node}:/tmp/
ssh {worker node}
sudo su
```

```sh
apparmor_parser -q /tmp/profile
apparmor_status
apparmor_status | grep 'very-secure'

# exit to work pc
```

```sh
mkdir /var/work/tests/artifacts/9/ -p
k create deployment apparmor -n apparmor --image nginx:1.19.2 --dry-run=client -o yaml >9.yaml
```

```yaml
# vim 9.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: apparmor
  name: apparmor
  namespace: apparmor
spec:
  replicas: 1
  selector:
    matchLabels:
      app: apparmor
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: apparmor
    spec:
      nodeSelector:                             # add it
        security: apparmor                      # add it
      securityContext:
        appArmorProfile:                        # add it
          type: Localhost                       # add it
          localhostProfile: very-secure         # add it
      containers:
      - image: nginx:1.19.2
        name: c1     # update
        resources: {}
status: {}
```

```sh
k apply -f 9.yaml
k get po -n apparmor
```

```text
NAME                        READY   STATUS             RESTARTS     AGE
apparmor-555d68c4d8-ntcgl   0/1     CrashLoopBackOff   1 (8s ago)   10s
```

```sh
k logs {apparmor-xxxx} -n apparmor
```

```text
/docker-entrypoint.sh: 13: /docker-entrypoint.sh: cannot create /dev/null: Permission denied
/docker-entrypoint.sh: No files found in /docker-entrypoint.d/, skipping configuration
2023/09/29 06:14:49 [emerg] 1#1: mkdir() "/var/cache/nginx/client_temp" failed (13: Permission denied)
nginx: [emerg] mkdir() "/var/cache/nginx/client_temp" failed (13: Permission denied)
```

```sh
k logs {apparmor-xxxx} -n apparmor>/var/work/tests/artifacts/9/log
```

## 10

```sh
kubectl config use-context cluster6-admin@cluster6
```

https://kubernetes.io/docs/tasks/configure-pod-container/security-context/

Add readOnlyRootFilesystem  and volumes to write

```yaml
# k edit  deployment  secure  -n secure

# add line to container level

securityContext: # add
   readOnlyRootFilesystem: true # add
   runAsGroup: 3000
   runAsUser: 3000
   allowPrivilegeEscalation: false
volumeMounts:  # to c1 container
          - mountPath: /tmp
            name: temp-vol

# add to spec level

volumes:
- emptyDir: {}
  name: temp-vol

```

Check while pod will be running

```yaml
# k edit  deployment  secure  -n secure
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: secure
  name: secure
  namespace: secure
spec:
  replicas: 1
  selector:
    matchLabels:
      app: secure
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: secure
    spec:
      containers:
      - command:
        - sh
        - -c
        - while true ; do  echo "$(date) i am working . c1 . $(id)";  sleep 10  ;done
        image: viktoruj/cks-lab
        imagePullPolicy: Always
        name: c1
        resources: {}
        securityContext:
          readOnlyRootFilesystem: true
          runAsGroup: 3000
          runAsUser: 3000
          allowPrivilegeEscalation: false
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /tmp
          name: temp-vol
      - command:
        - sh
        - -c
        - while true ; do  echo "$(date) i am working . c2 . $(id)";  sleep 10  ;done
        image: viktoruj/cks-lab
        imagePullPolicy: Always
        name: c2
        resources: {}
        securityContext:
          readOnlyRootFilesystem: true
          runAsGroup: 3000
          runAsUser: 3000
          allowPrivilegeEscalation: false
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      - command:
        - sh
        - -c
        - while true ; do  echo "$(date) i am working . c3 . $(id)";  sleep 10  ;done
        image: viktoruj/cks-lab
        imagePullPolicy: Always
        name: c3
        resources: {}
        securityContext:
          readOnlyRootFilesystem: true
          runAsGroup: 3000
          runAsUser: 3000
          allowPrivilegeEscalation: false
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
      volumes:
      - emptyDir: {}
        name: temp-vol
```

## 11

```sh
kubectl config use-context cluster6-admin@cluster6
```

```sh
k get sa dev  -n rbac-1
k get rolebindings.rbac.authorization.k8s.io  -n rbac-1 -o wide
```

```sh
k edit role dev -n rbac-1
```

```sh
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: dev
  namespace: rbac-1
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - create
  - watch #  update
  - list

```

```sh
k create role dev -n rbac-2 --resource configmaps --verb get,list
k create rolebinding  dev -n rbac-2 --serviceaccount rbac-1:dev --role dev
k run  dev-rbac -n rbac-1 --image viktoruj/cks-lab -o yaml --dry-run=client --command sleep 60000 > 11.yaml
```

https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/

```yaml
# vim 11.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: dev-rbac
  name: dev-rbac
  namespace: rbac-1
spec:
  serviceAccountName: dev              # add it
  containers:
  - command:
    - sleep
    - "60000"
    image: viktoruj/cks-lab
    name: dev-rbac
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

```sh
k apply -f 11.yaml
k get po -n rbac-1
```

## 12

```sh
kubectl config use-context cluster7-admin@cluster7
```

```sh
k get no
ssh {work node}
```

```sh
sysdig --help
sysdig --list
sysdig --list | grep container
sysdig --list | grep user
sysdig --list | grep time
sysdig --list | grep k8s
```

```sh
sysdig  -p"%evt.time,%container.id,%container.name,%user.name,%k8s.ns.name,%k8s.pod.name"   container.image=docker.io/library/nginx:latest

sysdig  -p"%evt.time,%container.id,%container.name,%user.name,%k8s.ns.name,%k8s.pod.name"   container.image=docker.io/library/nginx:latest>/tmp/log
# wait 20 sec ,  and exit to worker pc
```

```sh
mkdir -p /var/work/tests/artifacts/12/
scp {work node }:/tmp/log /var/work/tests/artifacts/12/
```

## 13

```sh
kubectl config use-context cluster8-admin@cluster8
```

```sh
k get no
ssh {control-plane}
```

```sh
# check  admission_config.json
cat /etc/kubernetes/pki/admission_config.json
```

```sh
# check  admission_kube_config.yaml
cat /etc/kubernetes/pki/webhook/admission_kube_config.yaml
```

```yaml
# vim  /etc/kubernetes/pki/webhook/admission_kube_config.yaml
apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority: /etc/kubernetes/pki/webhook/server.crt
    server: https://image-bouncer-webhook:30020/image_policy  # add
  name: bouncer_webhook
contexts:
- context:
    cluster: bouncer_webhook
    user: api-server
  name: bouncer_validator
current-context: bouncer_validator
preferences: {}
users:
- name: api-server
  user:
    client-certificate: /etc/kubernetes/pki/apiserver.crt
    client-key:  /etc/kubernetes/pki/apiserver.key
```

```yaml
# vim /etc/kubernetes/manifests/kube-apiserver.yaml
# add to api parametrs

- --enable-admission-plugins=NodeRestriction,ImagePolicyWebhook
- --admission-control-config-file=/etc/kubernetes/pki/admission_config.json
```

```sh
service kubelet restart

# exit to work pc
```

```sh
k run test-tag --image nginx
```

```text
Error from server (Forbidden): pods "test-tag" is forbidden: image policy webhook backend denied one or more images: Images using latest tag are not allowed

```

```sh
k run test-tag --image nginx:alpine3.17
k get po test-tag
```

```text
NAME       READY   STATUS    RESTARTS   AGE
test-tag   1/1     Running   0          4m47s

```

## 14

```Dockerfile
# vim /var/work/14/Dockerfile

FROM ubuntu:20.04
RUN apt-get update
RUN apt-get -y install curl
RUN groupadd myuser
RUN useradd  -g myuser  myuser
USER myuser
CMD ["sh", "-c", "while true ; do  id ;  sleep 1  ;done"]
```

```sh
podman build . -t cks:14

podman  run -d --name  cks-14   cks:14
sleep 2
podman logs cks-14 | grep myuser
```

```sh
podman stop cks-14
podman rm  cks-14
```

## 15

```sh
kubectl config use-context    cluster6-admin@cluster6
```

https://kubernetes.io/docs/tutorials/security/ns-level-pss/

```sh
k get ns team-red --show-labels

kubectl label --overwrite ns team-red  pod-security.kubernetes.io/enforce=baseline

k get ns team-red --show-labels
```

```sh
k get po -n team-red
# delete all pods in ns team-red

k delete po {pod_names} -n  team-red --force
```

```sh
k get po -n team-red

# No resources found in team-red namespace.
```

```sh
k  events  replicasets.apps  -n team-red
mkdir /var/work/tests/artifacts/15 -p
k  events  replicasets.apps  -n team-red > /var/work/tests/artifacts/15/logs
```

## 16

```sh
kubectl config use-context cluster1-admin@cluster1
```

https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/

```sh
openssl genrsa -out myuser.key 2048
openssl req -new -key myuser.key -out myuser.csr
```

```yaml
cat <<EOF > CSR.yaml
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: john-developer # add
spec:
  request: $(cat myuser.csr | base64 | tr -d "\n")
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth
  - digital signature
  - key encipherment
EOF
```

```sh
k create ns development
k apply -f  CSR.yaml
k get csr
k certificate approve john-developer
k create role developer --resource=pods --verb=create,list,get --namespace=development
k  create rolebinding developer-role-binding --role=developer --user=john --namespace=development
k  auth can-i update pods --as=john --namespace=development
```

## 17

```sh
kubectl config use-context cluster9-admin@cluster9
```

```sh
k get crd
k get constraint
k get constrainttemplates
k edit constrainttemplates k8strustedimages
```

```yaml
.......
  - rego: |
      package k8strustedimages

      violation[{"msg": msg}] {
       not images
       msg := "not trusted image!"
      }

      images {
        image := input.review.object.spec.containers[_].image
        not startswith(image, "docker-fake.io/")
        not startswith(image, "google-gcr-fake.com/")
        not startswith(image, "very-bad-registry.com/")  # add
      }
...........
```

## 18

```sh
kubectl config use-context cluster10-admin@cluster10
```

https://kubernetes.io/docs/tutorials/security/seccomp/

```sh
k get no
ssh {work node}
```

```sh
sudo su

mkdir /var/lib/kubelet/seccomp -p
cp /var/work/profile-nginx.json /var/lib/kubelet/seccomp/

# exit to work pc
```

```sh
k run seccomp --image nginx -o yaml --dry-run=client > 18.yaml
```

```yaml
# vim 18.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: seccomp
  name: seccomp
spec:
  securityContext:                          # add
    seccompProfile:                         # add
      type: Localhost                       # add
      localhostProfile: profile-nginx.json  # add
  containers:
  - image: nginx
    name: seccomp
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
```

```sh
k apply -f 18.yaml
k get po seccomp
```

